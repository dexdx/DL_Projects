{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "constant-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc_practical_prologue\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "modified-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = dlc_practical_prologue.generate_pair_sets(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "developing-liberal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Weight-sharing \"Siamese\" LeNet\n",
    "class Siamese(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Siamese, self).__init__()\n",
    "        \n",
    "        self.LeNet1 = nn.Sequential(\n",
    "            nn.Conv2d(1,16,5),  # 16x10x10 (input is 1x14x14)\n",
    "            nn.MaxPool2d(2),    # 16x5x5\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,32,2), # 32x4x4\n",
    "            nn.MaxPool2d(2),    # 32x2x2 (-> 1x128 before LeNet2)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.LeNet2 = nn.Sequential(\n",
    "            nn.Linear(128,64),  # 1x64\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),   # 1x32\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.LeNet3 = nn.Sequential(\n",
    "            nn.Linear(32,16),   # 1x16\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(16,2)     # 1x2\n",
    "        )\n",
    "        \n",
    "    def forward_bro(self, x):\n",
    "        x = self.LeNet1(x)\n",
    "        x = x.view(-1,1,128)\n",
    "        x = self.LeNet2(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.forward_bro(x1)\n",
    "        x2 = self.forward_bro(x2)\n",
    "        x3 = x1 + x2\n",
    "        x3 = self.LeNet3(x3)\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "designed-diabetes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# control convolutions' dimensions\n",
    "x1 = train_input.narrow(0,0,100)\n",
    "x1 = x1[:,0].view(100,1,14,14)\n",
    "weight1 = torch.empty(16,1,5,5).normal_()\n",
    "bias1 = torch.empty(16).normal_()\n",
    "x1 = F.conv2d(x1, weight1, bias1)\n",
    "\n",
    "x1 = F.max_pool2d(x1, 2)\n",
    "x1 = F.relu(x1)\n",
    "\n",
    "weight2 = torch.empty(32,16,2,2).normal_()\n",
    "bias2 = torch.empty(32).normal_()\n",
    "x1 = F.conv2d(x1, weight2, bias2)\n",
    "\n",
    "x1 = F.max_pool2d(x1, 2)\n",
    "x1 = F.relu(x1)\n",
    "\n",
    "x1 = x1.view(-1,1,128)\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dirty-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, batch_size, nb_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "        for b in range(0, train_input.size(0), batch_size):\n",
    "            imgs = train_input.narrow(0, b, batch_size)\n",
    "            imgs1 = imgs[:,0].view(batch_size, 1, 14, 14)\n",
    "            imgs2 = imgs[:,1].view(batch_size, 1, 14, 14)\n",
    "            output = model(imgs1, imgs2).view(batch_size, -1)\n",
    "            loss = criterion(output, train_target.narrow(0, b, batch_size))\n",
    "            acc_loss += loss.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(e, acc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "expressed-pizza",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.923598349094391\n",
      "1 6.884099900722504\n",
      "2 6.88298773765564\n",
      "3 6.881442368030548\n",
      "4 6.880037546157837\n",
      "5 6.878948152065277\n",
      "6 6.876945734024048\n",
      "7 6.874556303024292\n",
      "8 6.891939699649811\n",
      "9 6.872559428215027\n",
      "10 6.876327812671661\n",
      "11 6.880187153816223\n",
      "12 6.8601542711257935\n",
      "13 6.860633730888367\n",
      "14 6.8835413455963135\n",
      "15 6.880693435668945\n",
      "16 6.878857970237732\n",
      "17 6.876905560493469\n",
      "18 6.883996605873108\n",
      "19 6.875350892543793\n",
      "20 6.875253915786743\n",
      "21 6.878506720066071\n",
      "22 6.875382959842682\n",
      "23 6.854404807090759\n",
      "24 6.871258318424225\n",
      "25 6.853077232837677\n",
      "26 6.884506642818451\n",
      "27 6.831049859523773\n",
      "28 6.879334330558777\n",
      "29 6.873808741569519\n",
      "30 6.87993198633194\n",
      "31 6.842313647270203\n",
      "32 6.8692914843559265\n",
      "33 6.851314187049866\n",
      "34 6.822456777095795\n",
      "35 6.812077760696411\n",
      "36 6.836694061756134\n",
      "37 6.828101217746735\n",
      "38 6.801045179367065\n",
      "39 6.777437090873718\n",
      "40 6.782059013843536\n",
      "41 6.80307549238205\n",
      "42 6.863027632236481\n",
      "43 6.840596854686737\n",
      "44 6.803246557712555\n",
      "45 6.7331782579422\n",
      "46 6.68571013212204\n",
      "47 6.77890008687973\n",
      "48 6.812386929988861\n",
      "49 6.719954609870911\n",
      "50 6.653148949146271\n",
      "51 6.605019927024841\n",
      "52 6.64318585395813\n",
      "53 6.606089115142822\n",
      "54 6.563661158084869\n",
      "55 6.716100811958313\n",
      "56 6.593069314956665\n",
      "57 6.55621200799942\n",
      "58 6.505064964294434\n",
      "59 6.332185983657837\n",
      "60 6.458054542541504\n",
      "61 6.427110075950623\n",
      "62 6.313667476177216\n",
      "63 6.562762379646301\n",
      "64 6.081780970096588\n",
      "65 6.457703590393066\n",
      "66 6.227990925312042\n",
      "67 6.240410327911377\n",
      "68 5.9265100955963135\n",
      "69 6.12068098783493\n",
      "70 5.899761140346527\n",
      "71 5.715727269649506\n",
      "72 6.114501595497131\n",
      "73 6.0543829798698425\n",
      "74 5.774246662855148\n",
      "75 5.727792501449585\n",
      "76 6.225095301866531\n",
      "77 6.122691869735718\n",
      "78 5.626456081867218\n",
      "79 5.708272993564606\n",
      "80 5.430255055427551\n",
      "81 5.542183965444565\n",
      "82 5.70647394657135\n",
      "83 5.984793484210968\n",
      "84 6.029133856296539\n",
      "85 5.14244145154953\n",
      "86 5.59749436378479\n",
      "87 4.645179182291031\n",
      "88 5.687463015317917\n",
      "89 5.866840034723282\n",
      "90 4.86593359708786\n",
      "91 4.207704067230225\n",
      "92 5.315560162067413\n",
      "93 4.72052127122879\n",
      "94 5.4143513441085815\n",
      "95 3.885660231113434\n",
      "96 5.290716141462326\n",
      "97 3.8705602884292603\n",
      "98 4.585620611906052\n",
      "99 3.9246782064437866\n",
      "100 4.857954055070877\n",
      "101 3.2288408428430557\n",
      "102 4.900624841451645\n",
      "103 3.9761996269226074\n",
      "104 2.746273621916771\n",
      "105 5.388424217700958\n",
      "106 3.0679178088903427\n",
      "107 3.771171420812607\n",
      "108 4.175653398036957\n",
      "109 2.8470141142606735\n",
      "110 8.00008237361908\n",
      "111 4.1929367780685425\n",
      "112 2.8504765331745148\n",
      "113 3.4816964268684387\n",
      "114 2.5824172645807266\n",
      "115 3.397553861141205\n",
      "116 1.8956092447042465\n",
      "117 1.7324161529541016\n",
      "118 6.056873723864555\n",
      "119 3.392930820584297\n",
      "120 1.3512828536331654\n",
      "121 0.9852959290146828\n",
      "122 0.6064316518604755\n",
      "123 0.5034993141889572\n",
      "124 0.3997593466192484\n",
      "125 0.3547379691153765\n",
      "126 0.2666876595467329\n",
      "127 0.24016046896576881\n",
      "128 0.22459670528769493\n",
      "129 0.213075902312994\n",
      "130 0.2040632776916027\n",
      "131 0.1967511922121048\n",
      "132 0.19066470302641392\n",
      "133 0.18547624349594116\n",
      "134 0.1810150146484375\n",
      "135 0.1771224793046713\n",
      "136 0.17364324629306793\n",
      "137 0.17051660874858499\n",
      "138 0.16766722220927477\n",
      "139 0.16502340324223042\n",
      "140 0.16272898064926267\n",
      "141 0.16059321584179997\n",
      "142 0.15853634290397167\n",
      "143 0.1565929837524891\n",
      "144 0.15458551701158285\n",
      "145 0.15242463117465377\n",
      "146 0.15005909884348512\n",
      "147 0.14681116584688425\n",
      "148 0.14183747163042426\n",
      "149 0.13831180334091187\n",
      "150 0.1368247359059751\n",
      "151 0.12917706882581115\n",
      "152 0.10317881032824516\n",
      "153 0.10069309547543526\n",
      "154 0.09912143740803003\n",
      "155 0.09779875632375479\n",
      "156 0.09664036380127072\n",
      "157 0.09558235434815288\n",
      "158 0.09462618408724666\n",
      "159 0.09374964935705066\n",
      "160 0.09293608949519694\n",
      "161 0.0921825582627207\n",
      "162 0.09148114663548768\n",
      "163 0.0908296110574156\n",
      "164 0.09021568926982582\n",
      "165 0.08964196150191128\n",
      "166 0.08910054061561823\n",
      "167 0.08858678140677512\n",
      "168 0.0881050187163055\n",
      "169 0.08764363406226039\n",
      "170 0.08720609894953668\n",
      "171 0.08679222967475653\n",
      "172 0.08639433770440519\n",
      "173 0.08601538022048771\n",
      "174 0.08565812045708299\n",
      "175 0.08531076367944479\n",
      "176 0.08497846755199134\n",
      "177 0.08466533874161541\n",
      "178 0.08435997203923762\n",
      "179 0.08406745130196214\n",
      "180 0.08378757140599191\n",
      "181 0.08351816888898611\n",
      "182 0.08325840579345822\n",
      "183 0.08300705882720649\n",
      "184 0.08276773570105433\n",
      "185 0.08253477071411908\n",
      "186 0.08231084072031081\n",
      "187 0.08209365140646696\n",
      "188 0.08188419975340366\n",
      "189 0.08168224710971117\n",
      "190 0.08148673875257373\n",
      "191 0.0812966472003609\n",
      "192 0.08111368981190026\n",
      "193 0.0809349378105253\n",
      "194 0.08076281566172838\n",
      "195 0.08059554174542427\n",
      "196 0.08043338311836123\n",
      "197 0.0802754086907953\n",
      "198 0.08012203802354634\n",
      "199 0.07997428765520453\n",
      "200 0.07982805604115129\n",
      "201 0.07968730619177222\n",
      "202 0.07955046789720654\n",
      "203 0.07941591064445674\n",
      "204 0.0792862162925303\n",
      "205 0.079158985754475\n",
      "206 0.07903575338423252\n",
      "207 0.07891457574442029\n",
      "208 0.07879697671160102\n",
      "209 0.07868221215903759\n",
      "210 0.07856920058839023\n",
      "211 0.07845971861388534\n",
      "212 0.07835224678274244\n",
      "213 0.07824749581050128\n",
      "214 0.07814521924592555\n",
      "215 0.07804454409051687\n",
      "216 0.07794674090109766\n",
      "217 0.07785033551044762\n",
      "218 0.0777562321163714\n",
      "219 0.07766417565289885\n",
      "220 0.07757358415983617\n",
      "221 0.07748548977542669\n",
      "222 0.07739853067323565\n",
      "223 0.07731332536786795\n",
      "224 0.077229710877873\n",
      "225 0.07714776101056486\n",
      "226 0.07706736319232732\n",
      "227 0.07698854117188603\n",
      "228 0.07691109727602452\n",
      "229 0.07683490111958236\n",
      "230 0.07676022930536419\n",
      "231 0.0766866757767275\n",
      "232 0.07661433750763535\n",
      "233 0.07654299202840775\n",
      "234 0.07647307810839266\n",
      "235 0.07640403043478727\n",
      "236 0.07633637683466077\n",
      "237 0.07626963697839528\n",
      "238 0.07620420155581087\n",
      "239 0.07613943330943584\n",
      "240 0.07607557775918394\n",
      "241 0.07601287635043263\n",
      "242 0.07595072488766164\n",
      "243 0.07589032559189945\n",
      "244 0.07582995621487498\n",
      "245 0.07577064807992429\n",
      "246 0.07571242761332542\n",
      "247 0.07565455662552267\n",
      "248 0.07559770985972136\n",
      "249 0.07554143504239619\n"
     ]
    }
   ],
   "source": [
    "model = Siamese()\n",
    "train_model(model, train_input, train_target, 100, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "imposed-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, input_data, target_data, batch_size):\n",
    "    nb_errors = 0\n",
    "    \n",
    "    for b in range(0, input_data.size(0), batch_size):\n",
    "            imgs = input_data.narrow(0, b, batch_size)\n",
    "            target = target_data.narrow(0, b, batch_size)\n",
    "            imgs1 = imgs[:,0].view(batch_size, 1, 14, 14)\n",
    "            imgs2 = imgs[:,1].view(batch_size, 1, 14, 14)\n",
    "            output = model(imgs1, imgs2).view(batch_size, -1)\n",
    "            pred = output.max(1)[1]\n",
    "            nb_errors += (pred-target).abs().sum().item()\n",
    "    \n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "reliable-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors = compute_nb_errors(model, train_input, train_target, 100)\n",
    "test_errors = compute_nb_errors(model, test_input, test_target, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "noticed-jacket",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_errors/train_input.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "iraqi-grain",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.508"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_errors/test_input.size(0)\n",
    "# crazy overfit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
