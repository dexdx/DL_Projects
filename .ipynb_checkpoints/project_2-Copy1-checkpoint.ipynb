{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d22775e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import empty\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7610c8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty(10,10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa38494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I dont know why we need this?\n",
    "class Module ( object ):\n",
    "    \n",
    "    def forward (self , input_ ):\n",
    "        return input_\n",
    "    def backward (self, grad):\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "            \n",
    "    def param ( self ):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "published-third",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE(Module):\n",
    "    \n",
    "    def __init__(self, prev_module = None):\n",
    "        self.prev_module =  prev_module\n",
    "    \n",
    "    def set_truth(self,y_true):\n",
    "        self.y_true = y_true\n",
    "    \n",
    "    def forward (self , input_ ):\n",
    "        assert input_.shape[0] == self.y_true.shape[0], \"Batch size must match!\"\n",
    "        assert input_.shape[1] == self.y_true.shape[1], \"Input and output size must match!\"\n",
    "        self.curr_input = input_\n",
    "        return (self.y_true-input_).square().mean()\n",
    "    \n",
    "    def backward (self):\n",
    "        #Calculate gradient\n",
    "\n",
    "        grad = (-2 * (self.y_true-self.curr_input)).mean(0,True)\n",
    "\n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def param ( self ):\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfc21a8e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-122790b3f503>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-122790b3f503>\"\u001b[1;36m, line \u001b[1;32m33\u001b[0m\n\u001b[1;33m    dtanh =\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class ReLU(Module):\n",
    "    \n",
    "    def __init__(self, prev_module = None, next_param = None):\n",
    "        self.prev_module =  prev_module\n",
    "        self.curr_grad = 0 #Temporary\n",
    "\n",
    "    def forward (self , input_ ):\n",
    "        self.curr_grad = (input_ > 0)\n",
    "        return input_ * self.curr_grad\n",
    "        \n",
    "    def backward (self , gradwrtoutput):\n",
    "        #Calculate gradient\n",
    "        grad = self.curr_grad * gradwrtoutput\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def param ( self ):\n",
    "        return []\n",
    "    \n",
    "class Tanh(Module):\n",
    "    \n",
    "    def __init__(self, prev_module = None, next_param = None):\n",
    "        self.prev_module =  prev_module\n",
    "        self.curr_grad = 0 #Temporary\n",
    "\n",
    "    def forward (self , input_ ):\n",
    "        return input_.tanh()\n",
    "        \n",
    "    def backward (self , gradwrtoutput):\n",
    "        #Calculate gradient\n",
    "        dtanh = \n",
    "        grad = gradwrtoutput.tanh().pow(2).multiply(-1).add(1)\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def param ( self ):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b26728de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO there is a bug in here somewhere\n",
    "class FCC(Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, prev_module = None, lr=1e-1, N = None):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.prev_module =  prev_module\n",
    "        # Xavier initialization\n",
    "        self.weights = empty(input_size, output_size).normal_(0, math.sqrt(2/(input_size + output_size)))\n",
    "        self.bias = empty(1,output_size).fill_(0.1) #TODO better init\n",
    "        self.curr_input = 0\n",
    "        self.lr = lr\n",
    "        self.batch_size = 1\n",
    "\n",
    "    def forward (self , input_ ):\n",
    "        assert input_.shape[1] == self.input_size, \"Input size must match!\" \n",
    "        out = input_ @ (self.weights) \n",
    "        out += self.bias\n",
    "        assert out.shape[1] == self.output_size, \"Output size must match!\" \n",
    "        assert out.shape[0] == input_.shape[0], \"Batch size is not consistent!\"\n",
    "        self.curr_input = input_\n",
    "        self.batch_size = input_.shape[0]\n",
    "        return out\n",
    "        \n",
    "    def backward (self , gradwrtoutput):\n",
    "        #Calculate gradient\n",
    "        grad = gradwrtoutput @ (self.weights.T)\n",
    "        \n",
    "        #update weights\n",
    "        self.update(gradwrtoutput, self.lr)  #This is the correct version\n",
    "        #self.update(grad, self.lr)\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def update(self, gradwrtoutput, learning_rate):\n",
    "        self.weights -= learning_rate * ( self.curr_input.T @ gradwrtoutput ) / self.batch_size\n",
    "        self.bias -= learning_rate * gradwrtoutput.mean(0,True)\n",
    "        \n",
    "    def param ( self ):\n",
    "        return [self.weights, self.bias]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dafc973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_input.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8100c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_builder():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        layer0 = FCC(2,10)\n",
    "        self.layers.append(layer0)\n",
    "        layer1 = ReLU(layer0)\n",
    "        self.layers.append(layer1)\n",
    "        layer2 = FCC(10,1, prev_module = layer1)\n",
    "        self.layers.append(layer2)\n",
    "        layer3 = ReLU(layer2)\n",
    "        self.layers.append(layer3)\n",
    "        layer4 = LossMSE(layer3)\n",
    "        self.layers.append(layer4)\n",
    "    \n",
    "    def model_train(self,input_, g_truth):\n",
    "        curr = input_\n",
    "        self.layers[-1].set_truth(g_truth)\n",
    "        for layer in self.layers:\n",
    "            curr = layer.forward(curr)\n",
    "        self.layers[-1].backward()        \n",
    "        \n",
    "    def model_eval(self,input_):\n",
    "        curr = input_\n",
    "        for layer in self.layers[:-1]:\n",
    "            curr = layer.forward(curr)\n",
    "            #print(curr)\n",
    "        return curr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaaaabef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential():\n",
    "    \n",
    "    def __init__(self, layer_list, arguments, loss='MSE', lr = 1e-1):\n",
    "        self.layers = []\n",
    "        last_layer = None\n",
    "        for idx ,layer_name in enumerate(layer_list):\n",
    "            if(layer_name == 'FCC'):\n",
    "                assert arguments[idx] != [], \"FCC requires a tuple as input!\"\n",
    "                curr_layer = FCC(arguments[idx][0], arguments[idx][1], last_layer, lr=lr)\n",
    "                self.layers.append(curr_layer)\n",
    "                last_layer = curr_layer\n",
    "            elif(layer_name == 'ReLU'):\n",
    "                assert arguments[idx] == [], \"Relu requires no input!\"\n",
    "                curr_layer = ReLU(last_layer)\n",
    "                self.layers.append(curr_layer)\n",
    "                last_layer = curr_layer\n",
    "            elif(layer_name == 'Tanh'):\n",
    "                assert arguments[idx] == [], \"Tanh requires no input!\"\n",
    "                curr_layer = Tanh(last_layer)\n",
    "                self.layers.append(curr_layer)\n",
    "                last_layer = curr_layer\n",
    "            else:\n",
    "                raise Exception(\"No Module matches the input\")\n",
    "\n",
    "        if loss == 'MSE':\n",
    "            curr_layer = LossMSE(last_layer)\n",
    "            self.layers.append(curr_layer)\n",
    "        else:\n",
    "            raise Exception(\"No Loss matches the input\")\n",
    "                \n",
    "    def train(self,input_, g_truth):\n",
    "        out = input_\n",
    "        self.layers[-1].set_truth(g_truth)\n",
    "        for layer in self.layers[:-1]:\n",
    "            out = layer.forward(out)\n",
    "        loss = self.layers[-1].forward(out)\n",
    "        self.layers[-1].backward()  \n",
    "        return out,loss.mean()\n",
    "        \n",
    "    def eval(self,input_):\n",
    "        out = input_\n",
    "        for layer in self.layers[:-1]:\n",
    "            out = layer.forward(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d164496",
   "metadata": {},
   "source": [
    "### These two are just for a quick check I know they are terrible :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fbc57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stupid_test_function(a,b):\n",
    "    if a == 0 or b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06509a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stupid_acc_func(pred,true):\n",
    "    pred = pred.item() > 0.5\n",
    "    return (pred == true.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "registered-consideration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.27712157368659973\n",
      "Loss: 0.2940332591533661\n",
      "Loss: 0.27296182513237\n",
      "Loss: 0.29190176725387573\n",
      "Loss: 0.27189841866493225\n",
      "Loss: 0.2488432377576828\n",
      "Loss: 0.25251638889312744\n",
      "Loss: 0.2717173993587494\n",
      "Loss: 0.2822205424308777\n",
      "Loss: 0.2721976041793823\n",
      "Loss: 0.24939998984336853\n",
      "Loss: 0.3045389950275421\n",
      "Loss: 0.28020012378692627\n",
      "Loss: 0.2609328329563141\n",
      "Loss: 0.2984063923358917\n",
      "Loss: 0.26833605766296387\n",
      "Loss: 0.274880051612854\n",
      "Loss: 0.27065515518188477\n",
      "Loss: 0.2812103033065796\n",
      "Loss: 0.29319125413894653\n",
      "Loss: 0.28344205021858215\n",
      "Loss: 0.28310656547546387\n",
      "Loss: 0.23724153637886047\n",
      "Loss: 0.2635858356952667\n",
      "Loss: 0.2969098687171936\n",
      "Loss: 0.28209319710731506\n",
      "Loss: 0.3300948739051819\n",
      "Loss: 0.3171777129173279\n",
      "Loss: 0.26125314831733704\n",
      "Loss: 0.3138587474822998\n",
      "Loss: 0.2664629817008972\n",
      "Loss: 0.2954842448234558\n",
      "Loss: 0.2935682237148285\n",
      "Loss: 0.2871174216270447\n",
      "Loss: 0.24119794368743896\n",
      "Loss: 0.2591492831707001\n",
      "Loss: 0.2969052195549011\n",
      "Loss: 0.29262059926986694\n",
      "Loss: 0.294305682182312\n",
      "Loss: 0.289705753326416\n",
      "Loss: 0.29450052976608276\n",
      "Loss: 0.2774498462677002\n",
      "Loss: 0.28977566957473755\n",
      "Loss: 0.24017715454101562\n",
      "Loss: 0.2376060038805008\n",
      "Loss: 0.2728213369846344\n",
      "Loss: 0.21786464750766754\n",
      "Loss: 0.2255593091249466\n",
      "Loss: 0.27939391136169434\n",
      "Loss: 0.2546621561050415\n",
      "Loss: 0.23833520710468292\n",
      "Loss: 0.24605022370815277\n",
      "Loss: 0.257922887802124\n",
      "Loss: 0.27480125427246094\n",
      "Loss: 0.26636412739753723\n",
      "Loss: 0.266404926776886\n",
      "Loss: 0.2472267597913742\n",
      "Loss: 0.2572842538356781\n",
      "Loss: 0.2629242539405823\n",
      "Loss: 0.2497522532939911\n",
      "Loss: 0.24577122926712036\n",
      "Loss: 0.24954453110694885\n",
      "Loss: 0.23449359834194183\n",
      "Loss: 0.26122695207595825\n",
      "Loss: 0.24263156950473785\n",
      "Loss: 0.24900995194911957\n",
      "Loss: 0.23791176080703735\n",
      "Loss: 0.2604379951953888\n",
      "Loss: 0.2557189464569092\n",
      "Loss: 0.24181413650512695\n",
      "Loss: 0.23212985694408417\n",
      "Loss: 0.23313571512699127\n",
      "Loss: 0.2619614601135254\n",
      "Loss: 0.24383904039859772\n",
      "Loss: 0.2443259209394455\n",
      "Loss: 0.2745245397090912\n",
      "Loss: 0.23644395172595978\n",
      "Loss: 0.2310405671596527\n",
      "Loss: 0.22800788283348083\n",
      "Loss: 0.2564801275730133\n",
      "Loss: 0.2596062421798706\n",
      "Loss: 0.2437492161989212\n",
      "Loss: 0.2457055300474167\n",
      "Loss: 0.24956820905208588\n",
      "Loss: 0.23731593787670135\n",
      "Loss: 0.23360395431518555\n",
      "Loss: 0.2349369078874588\n",
      "Loss: 0.22824320197105408\n",
      "Loss: 0.26217329502105713\n",
      "Loss: 0.24623140692710876\n",
      "Loss: 0.2434912919998169\n",
      "Loss: 0.2504428029060364\n",
      "Loss: 0.24437753856182098\n",
      "Loss: 0.24224118888378143\n",
      "Loss: 0.2504158914089203\n",
      "Loss: 0.23564483225345612\n",
      "Loss: 0.24836374819278717\n",
      "Loss: 0.2389090359210968\n",
      "Loss: 0.23626548051834106\n",
      "Loss: 0.22594058513641357\n",
      "tensor([[0.4200]])\n",
      "tensor([[0.4298]])\n",
      "tensor([[0.3854]])\n",
      "tensor([[0.3824]])\n",
      "tensor([[0.3448]])\n",
      "tensor([[0.3328]])\n",
      "tensor([[0.3722]])\n",
      "tensor([[0.3952]])\n",
      "tensor([[0.4283]])\n",
      "tensor([[0.3931]])\n",
      "tensor([[0.3810]])\n",
      "tensor([[0.3600]])\n",
      "tensor([[0.4205]])\n",
      "tensor([[0.3642]])\n",
      "tensor([[0.4064]])\n",
      "tensor([[0.4416]])\n",
      "tensor([[0.3758]])\n",
      "tensor([[0.4379]])\n",
      "tensor([[0.3495]])\n",
      "tensor([[0.4285]])\n",
      "tensor([[0.4129]])\n",
      "tensor([[0.4118]])\n",
      "tensor([[0.3602]])\n",
      "tensor([[0.3752]])\n",
      "tensor([[0.4140]])\n",
      "tensor([[0.3773]])\n",
      "tensor([[0.4002]])\n",
      "tensor([[0.4115]])\n",
      "tensor([[0.4124]])\n",
      "tensor([[0.4359]])\n",
      "tensor([[0.4091]])\n",
      "tensor([[0.3735]])\n",
      "tensor([[0.4346]])\n",
      "tensor([[0.3915]])\n",
      "tensor([[0.4056]])\n",
      "tensor([[0.4279]])\n",
      "tensor([[0.4197]])\n",
      "tensor([[0.4199]])\n",
      "tensor([[0.3757]])\n",
      "tensor([[0.4185]])\n",
      "tensor([[0.3894]])\n",
      "tensor([[0.3717]])\n",
      "tensor([[0.3476]])\n",
      "tensor([[0.3995]])\n",
      "tensor([[0.3996]])\n",
      "tensor([[0.3449]])\n",
      "tensor([[0.4267]])\n",
      "tensor([[0.3649]])\n",
      "tensor([[0.3799]])\n",
      "tensor([[0.3522]])\n",
      "tensor([[0.3459]])\n",
      "tensor([[0.3479]])\n",
      "tensor([[0.4078]])\n",
      "tensor([[0.4263]])\n",
      "tensor([[0.4197]])\n",
      "tensor([[0.3500]])\n",
      "tensor([[0.3565]])\n",
      "tensor([[0.4276]])\n",
      "tensor([[0.4390]])\n",
      "tensor([[0.3985]])\n",
      "tensor([[0.4022]])\n",
      "tensor([[0.4421]])\n",
      "tensor([[0.3675]])\n",
      "tensor([[0.4231]])\n",
      "tensor([[0.3969]])\n",
      "tensor([[0.3732]])\n",
      "tensor([[0.4349]])\n",
      "tensor([[0.3692]])\n",
      "tensor([[0.4240]])\n",
      "tensor([[0.4401]])\n",
      "tensor([[0.4223]])\n",
      "tensor([[0.4397]])\n",
      "tensor([[0.3786]])\n",
      "tensor([[0.3636]])\n",
      "tensor([[0.3869]])\n",
      "tensor([[0.3739]])\n",
      "tensor([[0.4077]])\n",
      "tensor([[0.3740]])\n",
      "tensor([[0.4372]])\n",
      "tensor([[0.4015]])\n",
      "tensor([[0.3438]])\n",
      "tensor([[0.3516]])\n",
      "tensor([[0.4043]])\n",
      "tensor([[0.3729]])\n",
      "tensor([[0.3851]])\n",
      "tensor([[0.3820]])\n",
      "tensor([[0.3979]])\n",
      "tensor([[0.4304]])\n",
      "tensor([[0.4172]])\n",
      "tensor([[0.4367]])\n",
      "tensor([[0.4153]])\n",
      "tensor([[0.3896]])\n",
      "tensor([[0.4347]])\n",
      "tensor([[0.3686]])\n",
      "tensor([[0.4301]])\n",
      "tensor([[0.4412]])\n",
      "tensor([[0.3436]])\n",
      "tensor([[0.4065]])\n",
      "tensor([[0.3475]])\n",
      "tensor([[0.4384]])\n",
      "tensor([[0.4191]])\n",
      "tensor([[0.3533]])\n",
      "tensor([[0.4302]])\n",
      "tensor([[0.3485]])\n",
      "tensor([[0.3227]])\n",
      "tensor([[0.4136]])\n",
      "tensor([[0.4432]])\n",
      "tensor([[0.4051]])\n",
      "tensor([[0.3883]])\n",
      "tensor([[0.3559]])\n",
      "tensor([[0.4263]])\n",
      "tensor([[0.3595]])\n",
      "tensor([[0.4020]])\n",
      "tensor([[0.4384]])\n",
      "tensor([[0.4036]])\n",
      "tensor([[0.3843]])\n",
      "tensor([[0.3864]])\n",
      "tensor([[0.4014]])\n",
      "tensor([[0.3974]])\n",
      "tensor([[0.4434]])\n",
      "tensor([[0.3589]])\n",
      "tensor([[0.3925]])\n",
      "tensor([[0.3632]])\n",
      "tensor([[0.4045]])\n",
      "tensor([[0.3453]])\n",
      "tensor([[0.4408]])\n",
      "tensor([[0.4207]])\n",
      "tensor([[0.3905]])\n",
      "tensor([[0.4433]])\n",
      "tensor([[0.3649]])\n",
      "tensor([[0.3950]])\n",
      "tensor([[0.3540]])\n",
      "tensor([[0.3586]])\n",
      "tensor([[0.3927]])\n",
      "tensor([[0.4090]])\n",
      "tensor([[0.3491]])\n",
      "tensor([[0.4300]])\n",
      "tensor([[0.3909]])\n",
      "tensor([[0.3963]])\n",
      "tensor([[0.4120]])\n",
      "tensor([[0.3825]])\n",
      "tensor([[0.4076]])\n",
      "tensor([[0.4126]])\n",
      "tensor([[0.3645]])\n",
      "tensor([[0.3748]])\n",
      "tensor([[0.4390]])\n",
      "tensor([[0.3421]])\n",
      "tensor([[0.3963]])\n",
      "tensor([[0.3756]])\n",
      "tensor([[0.4108]])\n",
      "tensor([[0.4327]])\n",
      "tensor([[0.4162]])\n",
      "tensor([[0.3539]])\n",
      "tensor([[0.3291]])\n",
      "tensor([[0.4397]])\n",
      "tensor([[0.4139]])\n",
      "tensor([[0.3996]])\n",
      "tensor([[0.4028]])\n",
      "tensor([[0.4134]])\n",
      "tensor([[0.3403]])\n",
      "tensor([[0.4009]])\n",
      "tensor([[0.3721]])\n",
      "tensor([[0.3750]])\n",
      "tensor([[0.3400]])\n",
      "tensor([[0.4176]])\n",
      "tensor([[0.4090]])\n",
      "tensor([[0.3810]])\n",
      "tensor([[0.3971]])\n",
      "tensor([[0.4154]])\n",
      "tensor([[0.3892]])\n",
      "tensor([[0.4224]])\n",
      "tensor([[0.4226]])\n",
      "tensor([[0.3759]])\n",
      "tensor([[0.3648]])\n",
      "tensor([[0.3448]])\n",
      "tensor([[0.4314]])\n",
      "tensor([[0.3733]])\n",
      "tensor([[0.4345]])\n",
      "tensor([[0.4034]])\n",
      "tensor([[0.3485]])\n",
      "tensor([[0.4102]])\n",
      "tensor([[0.4234]])\n",
      "tensor([[0.4126]])\n",
      "tensor([[0.3983]])\n",
      "tensor([[0.3784]])\n",
      "tensor([[0.4006]])\n",
      "tensor([[0.3970]])\n",
      "tensor([[0.3744]])\n",
      "tensor([[0.4121]])\n",
      "tensor([[0.3818]])\n",
      "tensor([[0.3844]])\n",
      "tensor([[0.4422]])\n",
      "tensor([[0.3796]])\n",
      "tensor([[0.3527]])\n",
      "tensor([[0.3623]])\n",
      "tensor([[0.4250]])\n",
      "tensor([[0.3658]])\n",
      "tensor([[0.3952]])\n",
      "tensor([[0.3850]])\n",
      "tensor([[0.3574]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.555"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = Sequential([\"FCC\",\"Tanh\",\"FCC\",\"ReLU\", \"FCC\",\"ReLU\"],[[2,16],[], [16,8], [],[8,1], []],\"MSE\", 1e-2)\n",
    "train_size = 10000\n",
    "test_size = 200\n",
    "train_input = empty(train_size, 2).uniform_(0,1)\n",
    "train_target = train_input.add(-0.5).pow(2).sum(1).sub(1 / (7)).multiply(-1).sign().add(1).div(2)\n",
    "\n",
    "test_input = empty(test_size, 2).uniform_(0,1)\n",
    "test_target = test_input.add(-0.5).pow(2).sum(1).sub(1 / (7)).multiply(-1).sign().add(1).div(2)\n",
    "\n",
    "\n",
    "minibatch = 100\n",
    "for i in range(0, train_input.size(0), minibatch):\n",
    "    out,loss = seq.train(train_input.narrow(0, i, minibatch), train_target.narrow(0, i, minibatch).unsqueeze(1))\n",
    "    print(\"Loss:\", loss.item())\n",
    "acc = 0\n",
    "count = 0\n",
    "for i in range(test_size):\n",
    "    truth = test_target[i].unsqueeze(0).unsqueeze(1)\n",
    "    inp = test_input[i,:].unsqueeze(0)\n",
    "    out = seq.eval(inp)\n",
    "    print(out)\n",
    "    if stupid_acc_func(out,truth):\n",
    "        acc = acc + 1\n",
    "acc /test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f6f7578",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = NN_builder()\n",
    "test = empty(1000,2).random_(0,2)\n",
    "for i in range(1000):\n",
    "    truth = empty(1,1).fill_(stupid_test_function(test[i,0],test[i,1]))\n",
    "    inp = test[i,:].unsqueeze(0)\n",
    "    out = builder.model_train(inp,truth)\n",
    "    \n",
    "test2 = empty(100,2).random_(0,2)\n",
    "acc = 0\n",
    "count = 0\n",
    "for i in range(100):\n",
    "    truth = empty(1,1).fill_(stupid_test_function(test2[i,0],test2[i,1]))\n",
    "    inp = test2[i,:].unsqueeze(0)\n",
    "    out = builder.model_eval(inp)\n",
    "    #print(out)\n",
    "    if out.item() > 0.5:\n",
    "        count = count + 1\n",
    "    if stupid_acc_func(out,truth):\n",
    "        acc = acc + 1\n",
    "acc /100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "582a2e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0af08bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
