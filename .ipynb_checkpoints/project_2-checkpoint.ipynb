{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00e02a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ad47086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty(10,10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a5c0ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module ( object ):\n",
    "    \n",
    "    def param ( self ):\n",
    "        return [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b6e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE(Module):\n",
    "    \n",
    "    def __init__(self, prev_module = None):\n",
    "\n",
    "        self.prev_module =  prev_module\n",
    "        \n",
    "    def set_truth(self,y_true):\n",
    "        self.y_true = y_true\n",
    "        \n",
    "    def forward (self , input_ ):\n",
    "        assert input_.shape[1] == self.y_true.shape[1], \"Input and output size must match!\"\n",
    "        self.curr_input = input_\n",
    "        return (self.y_true-input_).square().mean()\n",
    "        \n",
    "    def backward (self):\n",
    "        #Calculate gradient\n",
    "        grad = -2 *(self.y_true-self.curr_input) / (self.curr_input.shape[1])        \n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def param ( self ):\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d21ae6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \n",
    "    def __init__(self, prev_module = None):\n",
    "        self.prev_module =  prev_module\n",
    "        self.curr_grad = 0 #Temporary\n",
    "\n",
    "    def forward (self , input_ ):\n",
    "        self.curr_grad = (input_ > 0)\n",
    "        return input_ * self.curr_grad\n",
    "        \n",
    "    def backward (self , gradwrtoutput):\n",
    "        #Calculate gradient\n",
    "        grad = self.curr_grad * gradwrtoutput\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def param ( self ):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edf577c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9f08b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCC(Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, prev_module = None):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.prev_module =  prev_module\n",
    "        self.weights = empty(input_size,output_size).fill_(0.1) #TODO better init\n",
    "        self.bias = empty(1,output_size).fill_(0.1) #TODO better init\n",
    "        self.curr_input = 0\n",
    "\n",
    "    def forward (self , input_ ):\n",
    "        assert input_.shape[1] == self.input_size, \"Input size must match!\" \n",
    "        #print(\"input size:\", input_.shape)\n",
    "        #print(\"self.weights size:\", self.weights.shape)\n",
    "        out = input_ @ (self.weights) \n",
    "        #print(\"biass size:\", self.bias.shape)\n",
    "        #print(\"out size:\", out.shape)\n",
    "        out += self.bias\n",
    "        assert out.shape[1] == self.output_size, \"Output size must match!\" \n",
    "        self.curr_input = input_\n",
    "        return out\n",
    "        \n",
    "    def backward (self , gradwrtoutput):\n",
    "        #Calculate gradient\n",
    "        grad = gradwrtoutput.multiply(self.weights)\n",
    "        \n",
    "        #update weights\n",
    "        self.update(gradwrtoutput,0.05) #TODO learning rate\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def update(self,gradwrtoutput,learning_rate):\n",
    "        self.weights -= learning_rate * ((self.curr_input.T) @ gradwrtoutput )\n",
    "        self.bias -= learning_rate * gradwrtoutput\n",
    "        \n",
    "    def param ( self ):\n",
    "        return [self.weights, self.bias]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f879e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_builder():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        layer0 = FCC(2,1)\n",
    "        self.layers.append(layer0)\n",
    "        layer1 = ReLU(layer0)\n",
    "        self.layers.append(layer1)\n",
    "        layer2 = LossMSE(layer1)\n",
    "        self.layers.append(layer2)\n",
    "    \n",
    "    def model_train(self,input_, g_truth):\n",
    "        curr = input_\n",
    "        self.layers[-1].set_truth(g_truth)\n",
    "        for layer in self.layers:\n",
    "            curr = layer.forward(curr)\n",
    "        self.layers[-1].backward()        \n",
    "        \n",
    "    def model_eval(self,input_):\n",
    "        curr = input_\n",
    "        for layer in self.layers[:-1]:\n",
    "            curr = layer.forward(curr)\n",
    "            #print(curr)\n",
    "        return curr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf52bf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = empty(1,2).fill_(5)\n",
    "truth = empty(1,1).fill_(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5fadac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "970bd9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = NN_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2db26527",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = builder.model_train(test,truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e97183c",
   "metadata": {},
   "source": [
    "### These two are just for a quick check I know they are terrible :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "05b6670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stupid_test_function(a,b):\n",
    "    if a == 0 or b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "54efc395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stupid_acc_func(pred,true):\n",
    "    pred = pred.item() > 0.9\n",
    "    return (pred == true.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ed9b8547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = NN_builder()\n",
    "test = empty(1000,2).random_(0,2)\n",
    "for i in range(1000):\n",
    "    truth = empty(1,1).fill_(stupid_test_function(test[i,0],test[i,1]))\n",
    "    inp = test[i,:].unsqueeze(0)\n",
    "    out = builder.model_train(inp,truth)\n",
    "    \n",
    "test2 = empty(100,2).random_(0,2)\n",
    "acc = 0\n",
    "count = 0\n",
    "for i in range(100):\n",
    "    truth = empty(1,1).fill_(stupid_test_function(test2[i,0],test2[i,1]))\n",
    "    inp = test2[i,:].unsqueeze(0)\n",
    "    out = builder.model_eval(inp)\n",
    "    if out.item() != 0:\n",
    "        count = count + 1\n",
    "    if stupid_acc_func(out,truth):\n",
    "        acc = acc + 1\n",
    "acc /100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "673b611c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4404de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"out:\", out.item() )\n",
    "    print(\"truth:\", truth.item() )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
