{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40973e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import empty\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6d52c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty(10,10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc456cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I dont know why we need this?\n",
    "class Module ( object ):\n",
    "    \n",
    "    def forward (self , input_ ):\n",
    "        return input_\n",
    "    def backward (self, grad):\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "            \n",
    "    def param ( self ):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1813d298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE(Module):\n",
    "    \n",
    "    def __init__(self, prev_module = None):\n",
    "        self.prev_module =  prev_module\n",
    "    \n",
    "    def set_truth(self,y_true):\n",
    "        self.y_true = y_true\n",
    "    \n",
    "    def forward (self , input_ ):\n",
    "        assert input_.shape[0] == self.y_true.shape[0], \"Batch size must match!\"\n",
    "        assert input_.shape[1] == self.y_true.shape[1], \"Input and output size must match!\"\n",
    "        self.curr_input = input_\n",
    "        return (self.y_true-input_).square().mean(1,True)  #Average per input not accross batches!\n",
    "    \n",
    "    def backward (self):\n",
    "        #Calculate gradient\n",
    "\n",
    "        grad = -2 *(self.y_true-self.curr_input) / (self.curr_input.shape[1])   #Divide by number of output samples not batch size     \n",
    "\n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def param ( self ):\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eddab210",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \n",
    "    def __init__(self, prev_module = None):\n",
    "        self.prev_module =  prev_module\n",
    "        self.curr_grad = 0 #Temporary\n",
    "\n",
    "    def forward (self , input_ ):\n",
    "        self.curr_grad = (input_ > 0)\n",
    "        return input_ * self.curr_grad\n",
    "        \n",
    "    def backward (self , gradwrtoutput):\n",
    "        #Calculate gradient\n",
    "        grad = self.curr_grad * gradwrtoutput\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def param ( self ):\n",
    "        return []\n",
    "    \n",
    "class Tanh(Module):\n",
    "    \n",
    "    def __init__(self, prev_module = None):\n",
    "        self.prev_module =  prev_module\n",
    "        self.curr_grad = 0 #Temporary\n",
    "\n",
    "    def forward (self , input_ ):\n",
    "        self.curr_grad = input_.tanh()\n",
    "        return self.curr_grad\n",
    "        \n",
    "    def backward (self , gradwrtoutput):\n",
    "        #Calculate gradient        \n",
    "        grad = self.curr_grad.tanh().pow(2).multiply(-1).add(1) * gradwrtoutput\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def param ( self ):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2027ef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO there is a bug in here somewhere\n",
    "class FCC(Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, prev_module = None, lr=1e-1, N = None,init_weights = None):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.prev_module =  prev_module\n",
    "\n",
    "        # Xavier initialization\n",
    "        self.weights = empty(input_size, output_size).normal_(0, math.sqrt(2/(input_size + output_size)))\n",
    "        #self.weights = empty(input_size, output_size).fill_(0.1)\n",
    "        self.initial_weights = self.weights\n",
    "        self.bias = empty(1,output_size).fill_(0.1) #TODO better init\n",
    "        self.curr_input = 0\n",
    "        self.lr = lr\n",
    "        self.batch_size = 1\n",
    "\n",
    "    def forward (self , input_ ):\n",
    "        assert input_.shape[1] == self.input_size, \"Input size must match!\" \n",
    "        out = input_ @ (self.weights) \n",
    "        out += self.bias\n",
    "        assert out.shape[1] == self.output_size, \"Output size must match!\" \n",
    "        assert out.shape[0] == input_.shape[0], \"Batch size is not consistent!\"\n",
    "        self.curr_input = input_\n",
    "        self.batch_size = input_.shape[0]\n",
    "        return out\n",
    "        \n",
    "    def backward (self , gradwrtoutput):\n",
    "        #Calculate gradient\n",
    "        grad = gradwrtoutput @ (self.weights.T)\n",
    "        \n",
    "        #update weights\n",
    "        self.update(gradwrtoutput, self.lr)  #This is the correct version\n",
    "        #self.update(grad, self.lr)\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def update(self, gradwrtoutput, learning_rate):\n",
    "        self.weights -= learning_rate * ( self.curr_input.T @ gradwrtoutput ) / self.batch_size\n",
    "        self.bias -= learning_rate * gradwrtoutput.mean(0,True)\n",
    "        \n",
    "    def param ( self ):\n",
    "        return [self.weights, self.bias]\n",
    "    \n",
    "    def initials(self):\n",
    "        return self.initial_weights\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "5ccf7d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_builder():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        layer0 = FCC(2,10)\n",
    "        self.layers.append(layer0)\n",
    "        layer1 = ReLU(layer0)\n",
    "        self.layers.append(layer1)\n",
    "        layer2 = FCC(10,1, prev_module = layer1)\n",
    "        self.layers.append(layer2)\n",
    "        layer3 = ReLU(layer2)\n",
    "        self.layers.append(layer3)\n",
    "        layer4 = LossMSE(layer3)\n",
    "        self.layers.append(layer4)\n",
    "    \n",
    "    def model_train(self,input_, g_truth):\n",
    "        curr = input_\n",
    "        self.layers[-1].set_truth(g_truth)\n",
    "        for layer in self.layers:\n",
    "            curr = layer.forward(curr)\n",
    "        self.layers[-1].backward()        \n",
    "        \n",
    "    def model_eval(self,input_):\n",
    "        curr = input_\n",
    "        for layer in self.layers[:-1]:\n",
    "            curr = layer.forward(curr)\n",
    "            #print(curr)\n",
    "        return curr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2aba266c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(object):\n",
    "    def __init__(self, layer_list, arguments, loss='MSE', lr = 2 * 1e-2):\n",
    "        self.layers = []\n",
    "        last_layer = None        \n",
    "        for idx ,layer_name in enumerate(layer_list):\n",
    "            if(layer_name == 'FCC'):\n",
    "                assert arguments[idx] != [], \"FCC requires a tuple as input!\"\n",
    "                curr_layer =FCC(arguments[idx][0], arguments[idx][1], last_layer, lr=lr)\n",
    "                self.inits = curr_layer.initials()\n",
    "                self.layers.append(curr_layer)\n",
    "                last_layer = curr_layer\n",
    "            elif(layer_name == 'ReLU'):\n",
    "                assert arguments[idx] == [], \"Relu requires no input!\"\n",
    "                curr_layer = ReLU(last_layer)\n",
    "                self.layers.append(curr_layer)\n",
    "                last_layer = curr_layer\n",
    "            elif(layer_name == 'Tanh'):\n",
    "                assert arguments[idx] == [], \"Tanh requires no input!\"\n",
    "                \n",
    "                curr_layer = Tanh(last_layer)\n",
    "                self.layers.append(curr_layer)\n",
    "                last_layer = curr_layer\n",
    "            else:\n",
    "                raise Exception(\"No Module matches the input\")\n",
    "\n",
    "        if loss == 'MSE':\n",
    "            curr_layer = LossMSE(last_layer)\n",
    "            self.layers.append(curr_layer)\n",
    "        else:\n",
    "            raise Exception(\"No Loss matches the input\")\n",
    "                \n",
    "    def train(self,input_, g_truth):\n",
    "        out = input_\n",
    "        self.layers[-1].set_truth(g_truth)\n",
    "        for layer in self.layers[:-1]:\n",
    "            out = layer.forward(out)\n",
    "        loss = self.layers[-1].forward(out)\n",
    "        self.layers[-1].backward()  \n",
    "        return out,loss.mean()\n",
    "        \n",
    "    def eval(self,input_):\n",
    "        out = input_\n",
    "        for layer in self.layers[:-1]:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "    \n",
    "    def get_inits(self):\n",
    "        return self.inits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3818347d",
   "metadata": {},
   "source": [
    "### These two are just for a quick check I know they are terrible :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "943e4ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stupid_test_function(a,b):\n",
    "    if a == 0 or b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d3ab350",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stupid_acc_func(pred,true):\n",
    "    pred = pred.item() > 0.5\n",
    "    return (pred == true.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418b7395",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a83a651b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x23f208e7208>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5968705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelTrainer(train_input,train_target,test_input,test_target,test_size):\n",
    "    seq = Sequential([\"FCC\",\"Tanh\",\"FCC\",\"ReLU\", \"FCC\",\"ReLU\"],[[2,16],[], [16,8], [],[8,1], []],\"MSE\")\n",
    "    for epoch in range(1000):\n",
    "        minibatch = 200\n",
    "        for i in range(0, train_input.size(0), minibatch):\n",
    "            out,loss = seq.train(train_input.narrow(0, i, minibatch), train_target.narrow(0, i, minibatch).unsqueeze(1))\n",
    "        if epoch %20 == 0:\n",
    "            print(\"Loss:\", loss.item())\n",
    "    print(\"---------------------------------\")\n",
    "    acc = 0\n",
    "    count = 0\n",
    "    for i in range(test_size):\n",
    "        truth = test_target[i].unsqueeze(0).unsqueeze(1)\n",
    "        inp = test_input[i,:].unsqueeze(0)\n",
    "        out = seq.eval(inp)\n",
    "        #print(out)\n",
    "        if stupid_acc_func(out,truth):\n",
    "            acc = acc + 1\n",
    "    return seq, acc /test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "896ecee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.27081426978111267\n",
      "Loss: 0.2519076466560364\n",
      "Loss: 0.2444741129875183\n",
      "Loss: 0.23900991678237915\n",
      "Loss: 0.23205886781215668\n",
      "Loss: 0.22607918083667755\n",
      "Loss: 0.2190430462360382\n",
      "Loss: 0.21228304505348206\n",
      "Loss: 0.20506973564624786\n",
      "Loss: 0.197510227560997\n",
      "Loss: 0.18955133855342865\n",
      "Loss: 0.18077094852924347\n",
      "Loss: 0.1721520572900772\n",
      "Loss: 0.1643044352531433\n",
      "Loss: 0.15666712820529938\n",
      "Loss: 0.14970679581165314\n",
      "Loss: 0.1434202343225479\n",
      "Loss: 0.13737870752811432\n",
      "Loss: 0.13184942305088043\n",
      "Loss: 0.12642164528369904\n",
      "Loss: 0.12115363776683807\n",
      "Loss: 0.11612387746572495\n",
      "Loss: 0.11094104498624802\n",
      "Loss: 0.10534810274839401\n",
      "Loss: 0.10110007226467133\n",
      "Loss: 0.0974506363272667\n",
      "Loss: 0.09444087743759155\n",
      "Loss: 0.0920797735452652\n",
      "Loss: 0.09020045399665833\n",
      "Loss: 0.08849503844976425\n",
      "Loss: 0.08710435032844543\n",
      "Loss: 0.08609186857938766\n",
      "Loss: 0.08520481735467911\n",
      "Loss: 0.08472247421741486\n",
      "Loss: 0.08391311764717102\n",
      "Loss: 0.08310465514659882\n",
      "Loss: 0.0816744789481163\n",
      "Loss: 0.08040180802345276\n",
      "Loss: 0.07960540801286697\n",
      "Loss: 0.07834019511938095\n",
      "Loss: 0.07747626304626465\n",
      "Loss: 0.07675328850746155\n",
      "Loss: 0.07651074975728989\n",
      "Loss: 0.07592929154634476\n",
      "Loss: 0.0752953365445137\n",
      "Loss: 0.07481894642114639\n",
      "Loss: 0.07381850481033325\n",
      "Loss: 0.0726061537861824\n",
      "Loss: 0.07173716276884079\n",
      "Loss: 0.07140544056892395\n",
      "---------------------------------\n",
      "Loss: 0.3374324440956116\n",
      "Loss: 0.24725942313671112\n",
      "Loss: 0.24221175909042358\n",
      "Loss: 0.23668941855430603\n",
      "Loss: 0.22922247648239136\n",
      "Loss: 0.22393614053726196\n",
      "Loss: 0.21848686039447784\n",
      "Loss: 0.21334928274154663\n",
      "Loss: 0.20874792337417603\n",
      "Loss: 0.2044520229101181\n",
      "Loss: 0.2006458342075348\n",
      "Loss: 0.1968899667263031\n",
      "Loss: 0.19341012835502625\n",
      "Loss: 0.19031865894794464\n",
      "Loss: 0.18736568093299866\n",
      "Loss: 0.18460477888584137\n",
      "Loss: 0.18195125460624695\n",
      "Loss: 0.17941442131996155\n",
      "Loss: 0.1765729784965515\n",
      "Loss: 0.173646941781044\n",
      "Loss: 0.17058047652244568\n",
      "Loss: 0.16773250699043274\n",
      "Loss: 0.16468749940395355\n",
      "Loss: 0.1611015945672989\n",
      "Loss: 0.15743683278560638\n",
      "Loss: 0.15300820767879486\n",
      "Loss: 0.1484556347131729\n",
      "Loss: 0.14234033226966858\n",
      "Loss: 0.13298369944095612\n",
      "Loss: 0.12041252851486206\n",
      "Loss: 0.10667511075735092\n",
      "Loss: 0.09358043968677521\n",
      "Loss: 0.08789338171482086\n",
      "Loss: 0.08552668243646622\n",
      "Loss: 0.08357034623622894\n",
      "Loss: 0.08222511410713196\n",
      "Loss: 0.08149423450231552\n",
      "Loss: 0.08112780749797821\n",
      "Loss: 0.08091947436332703\n",
      "Loss: 0.08066529035568237\n",
      "Loss: 0.08038870990276337\n",
      "Loss: 0.07991938292980194\n",
      "Loss: 0.079338438808918\n",
      "Loss: 0.07855390012264252\n",
      "Loss: 0.0780264139175415\n",
      "Loss: 0.07713167369365692\n",
      "Loss: 0.07572618126869202\n",
      "Loss: 0.07500516623258591\n",
      "Loss: 0.0740712583065033\n",
      "Loss: 0.07304501533508301\n",
      "---------------------------------\n",
      "Loss: 0.3012058138847351\n",
      "Loss: 0.24445553123950958\n",
      "Loss: 0.23314204812049866\n",
      "Loss: 0.225386843085289\n",
      "Loss: 0.21858267486095428\n",
      "Loss: 0.2116943746805191\n",
      "Loss: 0.20461370050907135\n",
      "Loss: 0.19708305597305298\n",
      "Loss: 0.1891481727361679\n",
      "Loss: 0.18050535023212433\n",
      "Loss: 0.17148111760616302\n",
      "Loss: 0.16252045333385468\n",
      "Loss: 0.15285836160182953\n",
      "Loss: 0.14315757155418396\n",
      "Loss: 0.13332776725292206\n",
      "Loss: 0.12380534410476685\n",
      "Loss: 0.11506611108779907\n",
      "Loss: 0.10750137269496918\n",
      "Loss: 0.10088253766298294\n",
      "Loss: 0.0953638106584549\n",
      "Loss: 0.09085062146186829\n",
      "Loss: 0.08756944537162781\n",
      "Loss: 0.08524478226900101\n",
      "Loss: 0.08372092992067337\n",
      "Loss: 0.0828649029135704\n",
      "Loss: 0.08247525244951248\n",
      "Loss: 0.08228854089975357\n",
      "Loss: 0.081830695271492\n",
      "Loss: 0.08153291791677475\n",
      "Loss: 0.08136792480945587\n",
      "Loss: 0.0812545046210289\n",
      "Loss: 0.08119595795869827\n",
      "Loss: 0.08117654174566269\n",
      "Loss: 0.08127136528491974\n",
      "Loss: 0.08133964240550995\n",
      "Loss: 0.08138281852006912\n",
      "Loss: 0.08147062361240387\n",
      "Loss: 0.08136170357465744\n",
      "Loss: 0.08127733319997787\n",
      "Loss: 0.08129481971263885\n",
      "Loss: 0.08134287595748901\n",
      "Loss: 0.08128255605697632\n",
      "Loss: 0.08124050498008728\n",
      "Loss: 0.08115849643945694\n",
      "Loss: 0.08039907366037369\n",
      "Loss: 0.07945418357849121\n",
      "Loss: 0.07909297943115234\n",
      "Loss: 0.07833974063396454\n",
      "Loss: 0.07769182324409485\n",
      "Loss: 0.0773368701338768\n",
      "---------------------------------\n",
      "Loss: 0.2878684103488922\n",
      "Loss: 0.24921028316020966\n",
      "Loss: 0.23846401274204254\n",
      "Loss: 0.22903066873550415\n",
      "Loss: 0.22062847018241882\n",
      "Loss: 0.2125990241765976\n",
      "Loss: 0.20491060614585876\n",
      "Loss: 0.19761475920677185\n",
      "Loss: 0.19095289707183838\n",
      "Loss: 0.18465782701969147\n",
      "Loss: 0.17948012053966522\n",
      "Loss: 0.17512953281402588\n",
      "Loss: 0.17140470445156097\n",
      "Loss: 0.16802197694778442\n",
      "Loss: 0.16515454649925232\n",
      "Loss: 0.16276590526103973\n",
      "Loss: 0.1608010083436966\n",
      "Loss: 0.15909405052661896\n",
      "Loss: 0.15760312974452972\n",
      "Loss: 0.15601962804794312\n",
      "Loss: 0.15411239862442017\n",
      "Loss: 0.15117231011390686\n",
      "Loss: 0.14796718955039978\n",
      "Loss: 0.14319854974746704\n",
      "Loss: 0.13815203309059143\n",
      "Loss: 0.1323881596326828\n",
      "Loss: 0.1264645904302597\n",
      "Loss: 0.12003213167190552\n",
      "Loss: 0.11269669234752655\n",
      "Loss: 0.10562737286090851\n",
      "Loss: 0.09901837259531021\n",
      "Loss: 0.09323035925626755\n",
      "Loss: 0.08826950192451477\n",
      "Loss: 0.08382356911897659\n",
      "Loss: 0.08033644407987595\n",
      "Loss: 0.07742564380168915\n",
      "Loss: 0.07519230246543884\n",
      "Loss: 0.07368488609790802\n",
      "Loss: 0.07270041108131409\n",
      "Loss: 0.07195336371660233\n",
      "Loss: 0.07138071954250336\n",
      "Loss: 0.07098603248596191\n",
      "Loss: 0.07053175568580627\n",
      "Loss: 0.07028768211603165\n",
      "Loss: 0.07001099735498428\n",
      "Loss: 0.06993666291236877\n",
      "Loss: 0.06974634528160095\n",
      "Loss: 0.0693822130560875\n",
      "Loss: 0.06901967525482178\n",
      "Loss: 0.06867394596338272\n",
      "---------------------------------\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "---------------------------------\n",
      "Loss: 0.46227315068244934\n",
      "Loss: 0.2256157249212265\n",
      "Loss: 0.21546527743339539\n",
      "Loss: 0.2059195190668106\n",
      "Loss: 0.19625477492809296\n",
      "Loss: 0.18651491403579712\n",
      "Loss: 0.1767559051513672\n",
      "Loss: 0.16714894771575928\n",
      "Loss: 0.15779094398021698\n",
      "Loss: 0.14893384277820587\n",
      "Loss: 0.14034011960029602\n",
      "Loss: 0.13241831958293915\n",
      "Loss: 0.12509211897850037\n",
      "Loss: 0.11834608018398285\n",
      "Loss: 0.11223134398460388\n",
      "Loss: 0.10702268779277802\n",
      "Loss: 0.10230333358049393\n",
      "Loss: 0.09823668748140335\n",
      "Loss: 0.09459257870912552\n",
      "Loss: 0.09147252887487411\n",
      "Loss: 0.08882096409797668\n",
      "Loss: 0.08667163550853729\n",
      "Loss: 0.08521177619695663\n",
      "Loss: 0.084214948117733\n",
      "Loss: 0.08339610695838928\n",
      "Loss: 0.08278567343950272\n",
      "Loss: 0.08226428180932999\n",
      "Loss: 0.08189871162176132\n",
      "Loss: 0.08146711438894272\n",
      "Loss: 0.08102923631668091\n",
      "Loss: 0.08059893548488617\n",
      "Loss: 0.08016696572303772\n",
      "Loss: 0.07956307381391525\n",
      "Loss: 0.07915279269218445\n",
      "Loss: 0.0786648839712143\n",
      "Loss: 0.07803044468164444\n",
      "Loss: 0.07746028900146484\n",
      "Loss: 0.07715633511543274\n",
      "Loss: 0.07692339271306992\n",
      "Loss: 0.07661332935094833\n",
      "Loss: 0.07630398124456406\n",
      "Loss: 0.07594771683216095\n",
      "Loss: 0.0754050761461258\n",
      "Loss: 0.07494955509901047\n",
      "Loss: 0.07450664788484573\n",
      "Loss: 0.07422579079866409\n",
      "Loss: 0.07393490523099899\n",
      "Loss: 0.073710136115551\n",
      "Loss: 0.07351097464561462\n",
      "Loss: 0.07335646450519562\n",
      "---------------------------------\n",
      "Loss: 0.31682848930358887\n",
      "Loss: 0.2189190834760666\n",
      "Loss: 0.21053054928779602\n",
      "Loss: 0.20236080884933472\n",
      "Loss: 0.19352516531944275\n",
      "Loss: 0.18414771556854248\n",
      "Loss: 0.1745186597108841\n",
      "Loss: 0.16485188901424408\n",
      "Loss: 0.15584532916545868\n",
      "Loss: 0.1471182405948639\n",
      "Loss: 0.1385658234357834\n",
      "Loss: 0.13063187897205353\n",
      "Loss: 0.12348102778196335\n",
      "Loss: 0.1174183189868927\n",
      "Loss: 0.11180127412080765\n",
      "Loss: 0.10711563378572464\n",
      "Loss: 0.10286787152290344\n",
      "Loss: 0.09959152340888977\n",
      "Loss: 0.09707622230052948\n",
      "Loss: 0.09496300667524338\n",
      "Loss: 0.09300077706575394\n",
      "Loss: 0.09214738756418228\n",
      "Loss: 0.09123551100492477\n",
      "Loss: 0.09083154797554016\n",
      "Loss: 0.09020928293466568\n",
      "Loss: 0.08978050202131271\n",
      "Loss: 0.08904501795768738\n",
      "Loss: 0.08768763393163681\n",
      "Loss: 0.08677506446838379\n",
      "Loss: 0.08532439172267914\n",
      "Loss: 0.08381270617246628\n",
      "Loss: 0.08318237960338593\n",
      "Loss: 0.08292082697153091\n",
      "Loss: 0.08276664465665817\n",
      "Loss: 0.08289998024702072\n",
      "Loss: 0.08304522186517715\n",
      "Loss: 0.08306100219488144\n",
      "Loss: 0.08303026109933853\n",
      "Loss: 0.08302052319049835\n",
      "Loss: 0.08311741799116135\n",
      "Loss: 0.08326277136802673\n",
      "Loss: 0.08334552496671677\n",
      "Loss: 0.08329769223928452\n",
      "Loss: 0.08327270299196243\n",
      "Loss: 0.08337786048650742\n",
      "Loss: 0.08353966474533081\n",
      "Loss: 0.08362551033496857\n",
      "Loss: 0.08359091728925705\n",
      "Loss: 0.08355863392353058\n",
      "Loss: 0.08348627388477325\n",
      "---------------------------------\n",
      "Loss: 0.2935495376586914\n",
      "Loss: 0.23151975870132446\n",
      "Loss: 0.22728782892227173\n",
      "Loss: 0.22336430847644806\n",
      "Loss: 0.21921750903129578\n",
      "Loss: 0.21352733671665192\n",
      "Loss: 0.20737458765506744\n",
      "Loss: 0.20001138746738434\n",
      "Loss: 0.19326674938201904\n",
      "Loss: 0.18568271398544312\n",
      "Loss: 0.17987310886383057\n",
      "Loss: 0.17514264583587646\n",
      "Loss: 0.17097589373588562\n",
      "Loss: 0.16778534650802612\n",
      "Loss: 0.1655205488204956\n",
      "Loss: 0.16418056190013885\n",
      "Loss: 0.16261543333530426\n",
      "Loss: 0.16202417016029358\n",
      "Loss: 0.16157476603984833\n",
      "Loss: 0.16020320355892181\n",
      "Loss: 0.15873821079730988\n",
      "Loss: 0.15824095904827118\n",
      "Loss: 0.15779250860214233\n",
      "Loss: 0.15716329216957092\n",
      "Loss: 0.15630604326725006\n",
      "Loss: 0.15568868815898895\n",
      "Loss: 0.15535376965999603\n",
      "Loss: 0.15458323061466217\n",
      "Loss: 0.15403012931346893\n",
      "Loss: 0.1531544327735901\n",
      "Loss: 0.15214131772518158\n",
      "Loss: 0.15113773941993713\n",
      "Loss: 0.14908240735530853\n",
      "Loss: 0.14722976088523865\n",
      "Loss: 0.1451168656349182\n",
      "Loss: 0.14190076291561127\n",
      "Loss: 0.13937951624393463\n",
      "Loss: 0.13411560654640198\n",
      "Loss: 0.12796176970005035\n",
      "Loss: 0.11997894942760468\n",
      "Loss: 0.11172639578580856\n",
      "Loss: 0.10394565761089325\n",
      "Loss: 0.09919503331184387\n",
      "Loss: 0.09542448818683624\n",
      "Loss: 0.09284038841724396\n",
      "Loss: 0.09127750992774963\n",
      "Loss: 0.09017447382211685\n",
      "Loss: 0.0891852006316185\n",
      "Loss: 0.08841229230165482\n",
      "Loss: 0.0878349021077156\n",
      "---------------------------------\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "Loss: 0.5099999904632568\n",
      "---------------------------------\n",
      "Loss: 0.26574209332466125\n",
      "Loss: 0.2549728453159332\n",
      "Loss: 0.2512458860874176\n",
      "Loss: 0.24942868947982788\n",
      "Loss: 0.24757519364356995\n",
      "Loss: 0.24552924931049347\n",
      "Loss: 0.24327120184898376\n",
      "Loss: 0.24078518152236938\n",
      "Loss: 0.23798343539237976\n",
      "Loss: 0.2348492592573166\n",
      "Loss: 0.2312542200088501\n",
      "Loss: 0.22715362906455994\n",
      "Loss: 0.22251863777637482\n",
      "Loss: 0.2173098921775818\n",
      "Loss: 0.2113334685564041\n",
      "Loss: 0.20479823648929596\n",
      "Loss: 0.19753387570381165\n",
      "Loss: 0.1903488039970398\n",
      "Loss: 0.1824253499507904\n",
      "Loss: 0.1741163432598114\n",
      "Loss: 0.16510911285877228\n",
      "Loss: 0.1562207192182541\n",
      "Loss: 0.1473715752363205\n",
      "Loss: 0.13897819817066193\n",
      "Loss: 0.13103023171424866\n",
      "Loss: 0.1234292984008789\n",
      "Loss: 0.1164613887667656\n",
      "Loss: 0.11055248230695724\n",
      "Loss: 0.10551811009645462\n",
      "Loss: 0.1013929545879364\n",
      "Loss: 0.09781470894813538\n",
      "Loss: 0.09486233443021774\n",
      "Loss: 0.09251273423433304\n",
      "Loss: 0.09085316956043243\n",
      "Loss: 0.08937402814626694\n",
      "Loss: 0.08823391050100327\n",
      "Loss: 0.08731549978256226\n",
      "Loss: 0.08652397990226746\n",
      "Loss: 0.0860113874077797\n",
      "Loss: 0.08532077819108963\n",
      "Loss: 0.08491754531860352\n",
      "Loss: 0.08467131853103638\n",
      "Loss: 0.08453135192394257\n",
      "Loss: 0.0844147652387619\n",
      "Loss: 0.08439020812511444\n",
      "Loss: 0.08434329926967621\n",
      "Loss: 0.08427596837282181\n",
      "Loss: 0.084464892745018\n",
      "Loss: 0.08451631665229797\n",
      "Loss: 0.08448324352502823\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.93"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = 1000\n",
    "test_size = 200\n",
    "valid_size = 100\n",
    "train_input = empty(train_size, 2).uniform_(0,1)\n",
    "train_target = train_input.add(-0.5).pow(2).sum(1).sub(1 / (7)).multiply(-1).sign().add(1).div(2)\n",
    "\n",
    "valid_input = empty(valid_size, 2).uniform_(0,1)\n",
    "valid_target = valid_input.add(-0.5).pow(2).sum(1).sub(1 / (7)).multiply(-1).sign().add(1).div(2)\n",
    "\n",
    "test_input = empty(test_size, 2).uniform_(0,1)\n",
    "test_target = test_input.add(-0.5).pow(2).sum(1).sub(1 / (7)).multiply(-1).sign().add(1).div(2)\n",
    "\n",
    "\n",
    "\n",
    "models = []\n",
    "best_acc = 0\n",
    "curr_acc = 0\n",
    "best_idx = -1\n",
    "for idx in range(10):    \n",
    "    model,curr_acc = modelTrainer(train_input,train_target,valid_input,valid_target,valid_size) \n",
    "    models.append(model)\n",
    "    if (curr_acc > best_acc):\n",
    "        best_acc = curr_acc\n",
    "        best_idx = idx\n",
    "\n",
    "acc = 0\n",
    "count = 0\n",
    "for i in range(test_size):\n",
    "    truth = test_target[i].unsqueeze(0).unsqueeze(1)\n",
    "    inp = test_input[i,:].unsqueeze(0)\n",
    "    out = models[best_idx].eval(inp)\n",
    "    #print(out)\n",
    "    if stupid_acc_func(out,truth):\n",
    "        acc = acc + 1\n",
    "acc /test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "1bd0d5cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ee6cb807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
       "        1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
       "        0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
       "        0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
       "        1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "        1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
       "        1., 0.])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece6ad42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ef5e8e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_weights = seq.get_inits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3ee329cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7084],\n",
       "        [ 0.1054],\n",
       "        [-1.2877],\n",
       "        [-0.9975],\n",
       "        [ 0.4085],\n",
       "        [-1.2745],\n",
       "        [-0.5271],\n",
       "        [-0.9473]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "7f29f45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = NN_builder()\n",
    "test = empty(1000,2).random_(0,2)\n",
    "for i in range(1000):\n",
    "    truth = empty(1,1).fill_(stupid_test_function(test[i,0],test[i,1]))\n",
    "    inp = test[i,:].unsqueeze(0)\n",
    "    out = builder.model_train(inp,truth)\n",
    "    \n",
    "test2 = empty(100,2).random_(0,2)\n",
    "acc = 0\n",
    "count = 0\n",
    "for i in range(100):\n",
    "    truth = empty(1,1).fill_(stupid_test_function(test2[i,0],test2[i,1]))\n",
    "    inp = test2[i,:].unsqueeze(0)\n",
    "    out = builder.model_eval(inp)\n",
    "    #print(out)\n",
    "    if out.item() > 0.5:\n",
    "        count = count + 1\n",
    "    if stupid_acc_func(out,truth):\n",
    "        acc = acc + 1\n",
    "acc /100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c9c19d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
