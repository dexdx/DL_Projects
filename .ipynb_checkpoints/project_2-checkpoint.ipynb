{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "provincial-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import empty\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "loaded-video",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty(10,10).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "twenty-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I dont know why we need this?\n",
    "class Module ( object ):\n",
    "    \n",
    "    def forward (self , input_ ):\n",
    "        return input_\n",
    "    def backward (self, grad):\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "            \n",
    "    def param ( self ):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "needed-passion",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE(Module):\n",
    "    \n",
    "    def __init__(self, prev_module = None):\n",
    "\n",
    "        self.prev_module =  prev_module\n",
    "        \n",
    "    def set_truth(self,y_true):\n",
    "        self.y_true = y_true\n",
    "        \n",
    "    def forward (self , input_ ):\n",
    "        assert input_.shape[1] == self.y_true.shape[1], \"Input and output size must match!\"\n",
    "        self.curr_input = input_\n",
    "        return (self.y_true-input_).square().mean()\n",
    "        \n",
    "    def backward (self):\n",
    "        #Calculate gradient\n",
    "        grad = -2 *(self.y_true-self.curr_input) / (self.curr_input.shape[1])        \n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def param ( self ):\n",
    "        return []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "separated-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \n",
    "    def __init__(self, prev_module = None):\n",
    "        self.prev_module =  prev_module\n",
    "        self.curr_grad = 0 #Temporary\n",
    "\n",
    "    def forward (self , input_ ):\n",
    "        self.curr_grad = (input_ > 0)\n",
    "        return input_ * self.curr_grad\n",
    "        \n",
    "    def backward (self , gradwrtoutput):\n",
    "        #Calculate gradient\n",
    "        grad = self.curr_grad * gradwrtoutput\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def param ( self ):\n",
    "        return []\n",
    "    \n",
    "class Tanh(Module):\n",
    "    \n",
    "    def __init__(self, prev_module = None):\n",
    "        self.prev_module =  prev_module\n",
    "        self.curr_grad = 0 #Temporary\n",
    "\n",
    "    def forward (self , input_ ):\n",
    "        self.curr_grad = input_.tanh().power(2).multiply(-1).sum(1)\n",
    "        return input_.tanh()\n",
    "        \n",
    "    def backward (self , gradwrtoutput):\n",
    "        #Calculate gradient\n",
    "        grad = self.curr_grad * gradwrtoutput\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def param ( self ):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "collect-crazy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCC(Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, prev_module = None, lr=1e-1, N = None):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.prev_module =  prev_module\n",
    "        # Xavier initialization\n",
    "        self.weights = empty(input_size, output_size).normal_(0, math.sqrt(2/(input_size + output_size)))\n",
    "        self.bias = empty(1,output_size).fill_(0.1) #TODO better init\n",
    "        self.curr_input = 0\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward (self , input_ ):\n",
    "        assert input_.shape[1] == self.input_size, \"Input size must match!\" \n",
    "        out = input_ @ (self.weights) \n",
    "        out += self.bias\n",
    "        print(input_.size(), self.weights.size(), out.size(), 'FCC forward')\n",
    "        assert out.shape[1] == self.output_size, \"Output size must match!\" \n",
    "        self.curr_input = input_\n",
    "        return out\n",
    "        \n",
    "    def backward (self , gradwrtoutput):\n",
    "        #Calculate gradient\n",
    "        grad = gradwrtoutput.matmul(self.weights.transpose(1, 0))\n",
    "        \n",
    "        #update weights\n",
    "#         self.update(gradwrtoutput, self.lr)\n",
    "        self.update(grad, self.lr)\n",
    "        \n",
    "        #Call backward() for previous module\n",
    "        if self.prev_module is not None:\n",
    "            prev_grads = self.prev_module.backward(grad)\n",
    "    \n",
    "    def update(self, gradwrtoutput, learning_rate):\n",
    "        print('Grad wrt output, weights, curr_input.T, ' )\n",
    "        print(gradwrtoutput.size(), self.weights.size(), self.curr_input.T.size(), 'FCC backward')\n",
    "        self.weights -= learning_rate * ( gradwrtoutput.multiply(self.curr_input.T) )\n",
    "        self.bias -= learning_rate * gradwrtoutput\n",
    "        \n",
    "    def param ( self ):\n",
    "        return [self.weights, self.bias]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "solved-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN_builder():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        layer0 = FCC(2,10)\n",
    "        self.layers.append(layer0)\n",
    "        layer1 = ReLU(layer0)\n",
    "        self.layers.append(layer1)\n",
    "        layer2 = FCC(10,1, prev_module = layer1)\n",
    "        self.layers.append(layer2)\n",
    "        layer3 = ReLU(layer2)\n",
    "        self.layers.append(layer3)\n",
    "        layer4 = LossMSE(layer3)\n",
    "        self.layers.append(layer4)\n",
    "    \n",
    "    def model_train(self,input_, g_truth):\n",
    "        curr = input_\n",
    "        self.layers[-1].set_truth(g_truth)\n",
    "        for layer in self.layers:\n",
    "            curr = layer.forward(curr)\n",
    "        self.layers[-1].backward()        \n",
    "        \n",
    "    def model_eval(self,input_):\n",
    "        curr = input_\n",
    "        for layer in self.layers[:-1]:\n",
    "            curr = layer.forward(curr)\n",
    "            #print(curr)\n",
    "        return curr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "twelve-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential():\n",
    "    \n",
    "    def __init__(self, layer_list, arguments, loss='MSE', lr = 1e-1):\n",
    "        self.layers = []\n",
    "        last_layer = None\n",
    "        for idx ,layer_name in enumerate(layer_list):\n",
    "            if(layer_name == 'FCC'):\n",
    "                assert arguments[idx] != [], \"FCC requires a tuple as input!\"\n",
    "                curr_layer =FCC(arguments[idx][0], arguments[idx][1], last_layer, lr=lr)\n",
    "                self.layers.append(curr_layer)\n",
    "                last_layer = curr_layer\n",
    "            elif(layer_name == 'ReLU'):\n",
    "                assert arguments[idx] == [], \"Relu requires no input!\"\n",
    "                curr_layer = ReLU(last_layer)\n",
    "                self.layers.append(curr_layer)\n",
    "                last_layer = curr_layer\n",
    "            elif(layer_name == 'Tanh'):\n",
    "                assert arguments[idx] == [], \"Tanh requires no input!\"\n",
    "                curr_layer = Tanh(last_layer)\n",
    "                self.layers.append(curr_layer)\n",
    "                last_layer = curr_layer\n",
    "            else:\n",
    "                raise Exception(\"No Module matches the input\")\n",
    "\n",
    "        if loss == 'MSE':\n",
    "            curr_layer = LossMSE(last_layer)\n",
    "            self.layers.append(curr_layer)\n",
    "        else:\n",
    "            raise Exception(\"No Loss matches the input\")\n",
    "                \n",
    "    def train(self,input_, g_truth):\n",
    "        out = input_\n",
    "        self.layers[-1].set_truth(g_truth)\n",
    "        for layer in self.layers[:-1]:\n",
    "            out = layer.forward(out)\n",
    "        loss = self.layers[-1].forward(out)\n",
    "        self.layers[-1].backward()  \n",
    "        return out,loss\n",
    "        \n",
    "    def eval(self,input_):\n",
    "        out = input_\n",
    "        for layer in self.layers[:-1]:\n",
    "            out = layer.forward(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "compact-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = empty(1,2).fill_(5)\n",
    "truth = empty(1,1).fill_(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "homeless-gabriel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.]])"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "sealed-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = NN_builder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "crucial-custom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2]) torch.Size([2, 10]) torch.Size([1, 10]) FCC forward\n",
      "torch.Size([1, 10]) torch.Size([10, 1]) torch.Size([1, 1]) FCC forward\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2.3078]])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = builder.model_eval(test)\n",
    "# print(out)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-place",
   "metadata": {},
   "source": [
    "### These two are just for a quick check I know they are terrible :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "lesbian-mathematics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stupid_test_function(a,b):\n",
    "    if a == 0 or b == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "nominated-offering",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stupid_acc_func(pred,true):\n",
    "    pred = pred.item() > 0.9\n",
    "    return (pred == true.item())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "finnish-occasion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2]) torch.Size([2, 10]) torch.Size([100, 10]) FCC forward\n",
      "torch.Size([100, 10]) torch.Size([10, 1]) torch.Size([100, 1]) FCC forward\n",
      "Grad wrt output, weights, curr_input.T, \n",
      "torch.Size([100, 10]) torch.Size([10, 1]) torch.Size([10, 100]) FCC backward\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (10) must match the size of tensor b (100) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-236-46495b517ec1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtest2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-229-3a26b3c56cc2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_, g_truth)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-83-3f9e969fd719>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#Call backward() for previous module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_module\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m             \u001b[0mprev_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-84-bd83d199e16d>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradwrtoutput)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;31m#Call backward() for previous module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_module\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mprev_grads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparam\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mself\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-227-e5b7da61ef54>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradwrtoutput)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;31m#update weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m#         self.update(gradwrtoutput, self.lr)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;31m#Call backward() for previous module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-227-e5b7da61ef54>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, gradwrtoutput, learning_rate)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Grad wrt output, weights, curr_input.T, '\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradwrtoutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurr_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'FCC backward'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m \u001b[0mgradwrtoutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurr_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradwrtoutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (10) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "seq = Sequential([\"FCC\",\"ReLU\",\"FCC\",\"ReLU\"],[[2,10],[], [10,1], []],\"MSE\")\n",
    "train_input = empty(100, 2).uniform_(0,1)\n",
    "train_target = train_input.add(-0.5).pow(2).sum(1).sub(1 / (2*math.pi)).multiply(-1).sign().add(1).div(2)\n",
    "\n",
    "test_input = empty(100, 2).uniform_(0,1)\n",
    "test_target = test_input.add(-0.5).pow(2).sum(1).sub(1 / (2*math.pi)).multiply(-1).sign().add(1).div(2)\n",
    "\n",
    "\n",
    "minibatch = 100\n",
    "for i in range(0, train_input.size(0), minibatch):\n",
    "    out,_ = seq.train(train_input.narrow(0, i, minibatch), truth)\n",
    "    \n",
    "test2 = empty(100,2).random_(0,2)\n",
    "acc = 0\n",
    "count = 0\n",
    "for i in range(100):\n",
    "    truth = empty(1,1).fill_(stupid_test_function(test2[i,0],test2[i,1]))\n",
    "    inp = test2[i,:].unsqueeze(0)\n",
    "    out = seq.eval(inp)\n",
    "    if out.item() != 0:\n",
    "        count = count + 1\n",
    "    if stupid_acc_func(out,truth):\n",
    "        acc = acc + 1\n",
    "acc /100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "floppy-mounting",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = NN_builder()\n",
    "test = empty(1000,2).random_(0,2)\n",
    "for i in range(1000):\n",
    "    truth = empty(1,1).fill_(stupid_test_function(test[i,0],test[i,1]))\n",
    "    inp = test[i,:].unsqueeze(0)\n",
    "    out = builder.model_train(inp,truth)\n",
    "    \n",
    "test2 = empty(100,2).random_(0,2)\n",
    "acc = 0\n",
    "count = 0\n",
    "for i in range(100):\n",
    "    truth = empty(1,1).fill_(stupid_test_function(test2[i,0],test2[i,1]))\n",
    "    inp = test2[i,:].unsqueeze(0)\n",
    "    out = builder.model_eval(inp)\n",
    "    if out.item() != 0:\n",
    "        count = count + 1\n",
    "    if stupid_acc_func(out,truth):\n",
    "        acc = acc + 1\n",
    "acc /100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
