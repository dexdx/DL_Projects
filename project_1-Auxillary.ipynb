{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d1e917e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc_practical_prologue\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60f0d3",
   "metadata": {},
   "source": [
    "# AUXILLARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ec59a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = dlc_practical_prologue.generate_pair_sets(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b561ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight-sharing \"Siamese\" LeNet\n",
    "class Siamese(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Siamese, self).__init__()\n",
    "        \n",
    "        self.LeNet1 = nn.Sequential(\n",
    "            nn.Conv2d(1,16,5),  # 16x10x10 (input is 1x14x14)\n",
    "            nn.MaxPool2d(2),    # 16x5x5\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,32,2), # 32x4x4\n",
    "            nn.MaxPool2d(2),    # 32x2x2 (-> 1x128 before LeNet2)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.LeNet2 = nn.Sequential(\n",
    "            nn.Linear(128,64),  # 1x64\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),   # 1x32\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.LeNet3 = nn.Sequential(\n",
    "            nn.Linear(32,16),   # 1x16\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(16,2)     # 1x2\n",
    "        )\n",
    "        self.AuxLayer = nn.Sequential(\n",
    "            nn.Linear(32,16),   # 1x16\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(16,10)     # 1x10\n",
    "        )\n",
    "        \n",
    "    def forward_bro(self, x):\n",
    "        x = self.LeNet1(x)\n",
    "        x = x.view(-1,1,128)\n",
    "        x = self.LeNet2(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.forward_bro(x1)\n",
    "        x2 = self.forward_bro(x2)\n",
    "        x3 = x2 - x1\n",
    "        x1 = self.AuxLayer(x1)\n",
    "        x2 = self.AuxLayer(x2)\n",
    "        x3 = self.LeNet3(x3)\n",
    "        return x1,x2,x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb3ba2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 128])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# control convolutions' dimensions\n",
    "x1 = train_input.narrow(0,0,100)\n",
    "x1 = x1[:,0].view(100,1,14,14)\n",
    "weight1 = torch.empty(16,1,5,5).normal_()\n",
    "bias1 = torch.empty(16).normal_()\n",
    "x1 = F.conv2d(x1, weight1, bias1)\n",
    "\n",
    "x1 = F.max_pool2d(x1, 2)\n",
    "x1 = F.relu(x1)\n",
    "\n",
    "weight2 = torch.empty(32,16,2,2).normal_()\n",
    "bias2 = torch.empty(32).normal_()\n",
    "x1 = F.conv2d(x1, weight2, bias2)\n",
    "\n",
    "x1 = F.max_pool2d(x1, 2)\n",
    "x1 = F.relu(x1)\n",
    "\n",
    "x1 = x1.view(-1,1,128)\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f009f4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a4d1abb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 9, 3, 3, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes.narrow(0, 0, 5)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e83e72e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 8],\n",
       "        [9, 2],\n",
       "        [3, 9],\n",
       "        ...,\n",
       "        [9, 9],\n",
       "        [4, 0],\n",
       "        [3, 0]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b213185",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target,train_classes, batch_size, nb_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    aux_criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "        for b in range(0, train_input.size(0), batch_size):\n",
    "            imgs = train_input.narrow(0, b, batch_size)\n",
    "            imgs1 = imgs[:,0].view(batch_size, 1, 14, 14)\n",
    "            imgs2 = imgs[:,1].view(batch_size, 1, 14, 14)\n",
    "            x1_pred, x2_pred , output = model(imgs1, imgs2)\n",
    "            x1_pred = x1_pred.view(batch_size, -1)\n",
    "            x2_pred = x2_pred.view(batch_size, -1)\n",
    "            output = output.view(batch_size, -1)\n",
    "            loss = criterion(output, train_target.narrow(0, b, batch_size))\n",
    "            loss_aux1 = aux_criterion(x1_pred,train_classes.narrow(0, b, batch_size)[:,0])\n",
    "            loss_aux1 += aux_criterion(x2_pred,train_classes.narrow(0, b, batch_size)[:,1])\n",
    "            loss = loss + loss_aux1\n",
    "            acc_loss += loss.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(e, acc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "533f8179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 52.199321269989014\n",
      "1 48.6403546333313\n",
      "2 44.026705741882324\n",
      "3 43.46076774597168\n",
      "4 37.433269739151\n",
      "5 33.358933210372925\n",
      "6 30.44285559654236\n",
      "7 28.416818380355835\n",
      "8 28.387378215789795\n",
      "9 23.477592706680298\n",
      "10 20.939395904541016\n",
      "11 20.029269456863403\n",
      "12 17.550909638404846\n",
      "13 19.082154989242554\n",
      "14 16.621403217315674\n",
      "15 13.245406746864319\n",
      "16 11.870068311691284\n",
      "17 22.739344000816345\n",
      "18 15.087915420532227\n",
      "19 10.917676329612732\n",
      "20 9.340040862560272\n",
      "21 8.107764482498169\n",
      "22 8.0348020195961\n",
      "23 6.467557907104492\n",
      "24 5.976473927497864\n"
     ]
    }
   ],
   "source": [
    "model = Siamese()\n",
    "train_model(model, train_input, train_target,train_classes, 100, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9433619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, input_data, target_data, batch_size):\n",
    "    nb_errors = 0\n",
    "    \n",
    "    for b in range(0, input_data.size(0), batch_size):\n",
    "            imgs = input_data.narrow(0, b, batch_size)\n",
    "            target = target_data.narrow(0, b, batch_size)\n",
    "            imgs1 = imgs[:,0].view(batch_size, 1, 14, 14)\n",
    "            imgs2 = imgs[:,1].view(batch_size, 1, 14, 14)\n",
    "            _,_, output = model(imgs1, imgs2)\n",
    "            output = output.view(batch_size, -1)\n",
    "            pred = output.max(1)[1]\n",
    "            nb_errors += (pred-target).abs().sum().item()\n",
    "    \n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54ed9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors = compute_nb_errors(model, train_input, train_target, 100)\n",
    "test_errors = compute_nb_errors(model, test_input, test_target, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bedba5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc: 0.984\n",
      "test acc: 0.893\n"
     ]
    }
   ],
   "source": [
    "print(\"train acc:\" , (train_input.size(0) - train_errors)/train_input.size(0))\n",
    "print(\"test acc:\" , (test_input.size(0) - test_errors)/test_input.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a293c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
