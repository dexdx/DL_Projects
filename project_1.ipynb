{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "constant-observation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlc_practical_prologue\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "modified-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, train_classes, test_input, test_target, test_classes = dlc_practical_prologue.generate_pair_sets(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "developing-liberal",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Weight-sharing \"Siamese\" LeNet\n",
    "class Siamese(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Siamese, self).__init__()\n",
    "        \n",
    "        self.LeNet1 = nn.Sequential(\n",
    "            nn.Conv2d(1,16,5),  # 16x10x10 (input is 1x14x14)\n",
    "            nn.MaxPool2d(2),    # 16x5x5\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16,32,2), # 32x4x4\n",
    "            nn.MaxPool2d(2),    # 32x2x2 (-> 1x128 before LeNet2)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.LeNet2 = nn.Sequential(\n",
    "            nn.Linear(128,64),  # 1x64\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,32),   # 1x32\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.LeNet3 = nn.Sequential(\n",
    "            nn.Linear(32,16),   # 1x16\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(16,2)     # 1x2\n",
    "        )\n",
    "        \n",
    "    def forward_bro(self, x):\n",
    "        x = self.LeNet1(x)\n",
    "        x = x.view(-1,1,128)\n",
    "        x = self.LeNet2(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.forward_bro(x1)\n",
    "        x2 = self.forward_bro(x2)\n",
    "        x3 = x2 - x1\n",
    "        x3 = self.LeNet3(x3)\n",
    "        return x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "designed-diabetes",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 128])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# control convolutions' dimensions\n",
    "x1 = train_input.narrow(0,0,100)\n",
    "x1 = x1[:,0].view(100,1,14,14)\n",
    "weight1 = torch.empty(16,1,5,5).normal_()\n",
    "bias1 = torch.empty(16).normal_()\n",
    "x1 = F.conv2d(x1, weight1, bias1)\n",
    "\n",
    "x1 = F.max_pool2d(x1, 2)\n",
    "x1 = F.relu(x1)\n",
    "\n",
    "weight2 = torch.empty(32,16,2,2).normal_()\n",
    "bias2 = torch.empty(32).normal_()\n",
    "x1 = F.conv2d(x1, weight2, bias2)\n",
    "\n",
    "x1 = F.max_pool2d(x1, 2)\n",
    "x1 = F.relu(x1)\n",
    "\n",
    "x1 = x1.view(-1,1,128)\n",
    "x1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dirty-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, batch_size, nb_epochs):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr = 1e-1)\n",
    "\n",
    "    for e in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "        for b in range(0, train_input.size(0), batch_size):\n",
    "            imgs = train_input.narrow(0, b, batch_size)\n",
    "            imgs1 = imgs[:,0].view(batch_size, 1, 14, 14)\n",
    "            imgs2 = imgs[:,1].view(batch_size, 1, 14, 14)\n",
    "            output = model(imgs1, imgs2).view(batch_size, -1)\n",
    "            loss = criterion(output, train_target.narrow(0, b, batch_size))\n",
    "            acc_loss += loss.item()\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(e, acc_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "expressed-pizza",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6.123622298240662\n",
      "1 4.720898121595383\n",
      "2 4.099395543336868\n",
      "3 3.461404412984848\n",
      "4 3.139276295900345\n",
      "5 3.180239260196686\n",
      "6 2.565578520298004\n",
      "7 2.4368543326854706\n",
      "8 2.3552765399217606\n",
      "9 1.9125934019684792\n",
      "10 2.3408721536397934\n",
      "11 1.9243998602032661\n",
      "12 1.4963824972510338\n",
      "13 1.6994921192526817\n",
      "14 1.1543678119778633\n",
      "15 0.9868257157504559\n",
      "16 1.796875\n",
      "17 2.146762691438198\n",
      "18 1.070250265300274\n",
      "19 0.7067676801234484\n",
      "20 0.9381244853138924\n",
      "21 1.1707879528403282\n",
      "22 1.0643237680196762\n",
      "23 0.8283866718411446\n",
      "24 0.4535798504948616\n",
      "25 0.397851618938148\n",
      "26 0.30486978217959404\n",
      "27 0.272602922283113\n",
      "28 0.24062591511756182\n",
      "29 0.2320688357576728\n",
      "30 0.22679789271205664\n",
      "31 0.222559267655015\n",
      "32 0.2189282588660717\n",
      "33 0.21520602330565453\n",
      "34 0.2062004692852497\n",
      "35 0.18021039571613073\n",
      "36 0.17198903718963265\n",
      "37 0.16893693478778005\n",
      "38 0.16660790611058474\n",
      "39 0.1646477752365172\n",
      "40 0.16297075292095542\n",
      "41 0.16151724103838205\n",
      "42 0.16024419711902738\n",
      "43 0.15911510167643428\n",
      "44 0.158104807138443\n",
      "45 0.1571925370953977\n",
      "46 0.1563669885508716\n",
      "47 0.15561508387327194\n",
      "48 0.15493080019950867\n",
      "49 0.15430541522800922\n",
      "50 0.15373268723487854\n",
      "51 0.15320536773651838\n",
      "52 0.15271827997639775\n",
      "53 0.15226868446916342\n",
      "54 0.15185216534882784\n",
      "55 0.1514646429568529\n",
      "56 0.15110391844063997\n",
      "57 0.1507684898097068\n",
      "58 0.15045570069923997\n",
      "59 0.15016311453655362\n",
      "60 0.1498895287513733\n",
      "61 0.1496329924557358\n",
      "62 0.14939204649999738\n",
      "63 0.14916580030694604\n",
      "64 0.14895211020484567\n",
      "65 0.14875148911960423\n",
      "66 0.14856225484982133\n",
      "67 0.1483839093707502\n",
      "68 0.1482155646663159\n",
      "69 0.1480565897654742\n",
      "70 0.1479061646386981\n",
      "71 0.14776389207690954\n",
      "72 0.1476293713785708\n",
      "73 0.14750201045535505\n",
      "74 0.14738119603134692\n",
      "75 0.14726647059433162\n",
      "76 0.14715753588825464\n",
      "77 0.14705412811599672\n",
      "78 0.1469555669464171\n",
      "79 0.14686205401085317\n",
      "80 0.14677277812734246\n",
      "81 0.1466876920312643\n",
      "82 0.1466065205167979\n",
      "83 0.14652903005480766\n",
      "84 0.14645531959831715\n",
      "85 0.14638480939902365\n",
      "86 0.1463174349628389\n",
      "87 0.1462532915174961\n",
      "88 0.14619198255240917\n",
      "89 0.14613340189680457\n",
      "90 0.1460772983264178\n",
      "91 0.14602379105053842\n",
      "92 0.14597213244996965\n",
      "93 0.14592275931499898\n",
      "94 0.1458756527863443\n",
      "95 0.1458304156549275\n",
      "96 0.1457870590966195\n",
      "97 0.1457454354967922\n",
      "98 0.14570566616021097\n",
      "99 0.14566731289960444\n",
      "100 0.14563067350536585\n",
      "101 0.1455954280681908\n",
      "102 0.14556166203692555\n",
      "103 0.14552918751724064\n",
      "104 0.14549803035333753\n",
      "105 0.14546800288371742\n",
      "106 0.14543930836953223\n",
      "107 0.1454114643856883\n",
      "108 0.14538501226343215\n",
      "109 0.1453592216130346\n",
      "110 0.14533468172885478\n",
      "111 0.14531084103509784\n",
      "112 0.14528791652992368\n",
      "113 0.14526600623503327\n",
      "114 0.1452446631155908\n",
      "115 0.14522422896698117\n",
      "116 0.14520444651134312\n",
      "117 0.14518548478372395\n",
      "118 0.14516708487644792\n",
      "119 0.14514941163361073\n",
      "120 0.14513216447085142\n",
      "121 0.14511573198251426\n",
      "122 0.14509967318736017\n",
      "123 0.1450842684134841\n",
      "124 0.14506951882503927\n",
      "125 0.14505504188127816\n",
      "126 0.14504115050658584\n",
      "127 0.14502765657380223\n",
      "128 0.14501473866403103\n",
      "129 0.14500211621634662\n",
      "130 0.14498994802124798\n",
      "131 0.14497817098163068\n",
      "132 0.14496685075573623\n",
      "133 0.14495572703890502\n",
      "134 0.14494517585262656\n",
      "135 0.1449348023161292\n",
      "136 0.14492474356666207\n",
      "137 0.14491511299274862\n",
      "138 0.14490548940375447\n",
      "139 0.144896503072232\n",
      "140 0.14488767506554723\n",
      "141 0.14487912110053003\n",
      "142 0.14487082255072892\n",
      "143 0.14486277173273265\n",
      "144 0.14485497353598475\n",
      "145 0.14484743727371097\n",
      "146 0.14484011102467775\n",
      "147 0.14483298338018358\n",
      "148 0.14482600102201104\n",
      "149 0.1448192757088691\n",
      "150 0.1448127303738147\n",
      "151 0.14480637479573488\n",
      "152 0.14480027509853244\n",
      "153 0.14479429181665182\n",
      "154 0.14478826010599732\n",
      "155 0.14478262583725154\n",
      "156 0.14477715478278697\n",
      "157 0.14477177755907178\n",
      "158 0.14476662781089544\n",
      "159 0.14476154185831547\n",
      "160 0.14475664752535522\n",
      "161 0.14475182979367673\n",
      "162 0.14474707562476397\n",
      "163 0.1447425144724548\n",
      "164 0.14473810163326561\n",
      "165 0.14473363058641553\n",
      "166 0.1447294671088457\n",
      "167 0.14472531992942095\n",
      "168 0.14472131454385817\n",
      "169 0.14471732429228723\n",
      "170 0.14471351425163448\n",
      "171 0.14470969117246568\n",
      "172 0.14470608765259385\n",
      "173 0.1447025767993182\n",
      "174 0.14469904149882495\n",
      "175 0.14469565614126623\n",
      "176 0.14469227311201394\n",
      "177 0.14468906028196216\n",
      "178 0.14468597318045795\n",
      "179 0.14468283066526055\n",
      "180 0.14467981504276395\n",
      "181 0.14467679103836417\n",
      "182 0.14467395097017288\n",
      "183 0.1446710501331836\n",
      "184 0.14466829900629818\n",
      "185 0.1446655266918242\n",
      "186 0.14466296322643757\n",
      "187 0.14466020092368126\n",
      "188 0.14465758414007723\n",
      "189 0.14465525490231812\n",
      "190 0.1446527149528265\n",
      "191 0.14465025975368917\n",
      "192 0.14464797312393785\n",
      "193 0.1446456250268966\n",
      "194 0.14464337122626603\n",
      "195 0.14464115095324814\n",
      "196 0.14463894278742373\n",
      "197 0.1446369094774127\n",
      "198 0.14463472925126553\n",
      "199 0.1446326661389321\n",
      "200 0.14463066356256604\n",
      "201 0.14462872897274792\n",
      "202 0.14462690730579197\n",
      "203 0.14462479460053146\n",
      "204 0.14462286210618913\n",
      "205 0.14462104788981378\n",
      "206 0.14461924065835774\n",
      "207 0.14461762900464237\n",
      "208 0.1446158525068313\n",
      "209 0.14461410604417324\n",
      "210 0.14461236377246678\n",
      "211 0.14461075887084007\n",
      "212 0.14460902172140777\n",
      "213 0.1446075104176998\n",
      "214 0.14460584544576705\n",
      "215 0.14460434229113162\n",
      "216 0.1446027464699\n",
      "217 0.1446012845262885\n",
      "218 0.1445997531991452\n",
      "219 0.14459834014996886\n",
      "220 0.1445968362968415\n",
      "221 0.144595397869125\n",
      "222 0.14459406677633524\n",
      "223 0.14459256548434496\n",
      "224 0.1445914194919169\n",
      "225 0.14458984415978193\n",
      "226 0.14458854589611292\n",
      "227 0.14458732050843537\n",
      "228 0.144585938192904\n",
      "229 0.14458467904478312\n",
      "230 0.14458349882625043\n",
      "231 0.14458226668648422\n",
      "232 0.14458096865564585\n",
      "233 0.14457967784255743\n",
      "234 0.1445784952957183\n",
      "235 0.14457743428647518\n",
      "236 0.14457626873627305\n",
      "237 0.14457514882087708\n",
      "238 0.14457401097752154\n",
      "239 0.14457297138869762\n",
      "240 0.14457174809649587\n",
      "241 0.14457066007889807\n",
      "242 0.14456965122371912\n",
      "243 0.14456854690797627\n",
      "244 0.14456743118353188\n",
      "245 0.14456651313230395\n",
      "246 0.14456536807119846\n",
      "247 0.14456432196311653\n",
      "248 0.1445632365066558\n",
      "249 0.14456250448711216\n"
     ]
    }
   ],
   "source": [
    "model = Siamese()\n",
    "train_model(model, train_input, train_target, 100, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "imposed-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_errors(model, input_data, target_data, batch_size):\n",
    "    nb_errors = 0\n",
    "    \n",
    "    for b in range(0, input_data.size(0), batch_size):\n",
    "            imgs = input_data.narrow(0, b, batch_size)\n",
    "            target = target_data.narrow(0, b, batch_size)\n",
    "            imgs1 = imgs[:,0].view(batch_size, 1, 14, 14)\n",
    "            imgs2 = imgs[:,1].view(batch_size, 1, 14, 14)\n",
    "            output = model(imgs1, imgs2).view(batch_size, -1)\n",
    "            pred = output.max(1)[1]\n",
    "            nb_errors += (pred-target).abs().sum().item()\n",
    "    \n",
    "    return nb_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "reliable-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors = compute_nb_errors(model, train_input, train_target, 100)\n",
    "test_errors = compute_nb_errors(model, test_input, test_target, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "noticed-jacket",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.002"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_errors/train_input.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "iraqi-grain",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.165"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_errors/test_input.size(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
